# SENTENCE-LEVEL STREAMING STRATEGY FOR SESAME TTS

## Problem
Sesame model generates COMPLETE audio for entire text input.
Long text (50+ words) = 30-60 seconds to generate.

## Solution: Sentence-Level Chunking

Instead of sending entire response to Sesame:
"Well, uh, I don't know if you could help me out. I was wondering if you could possibly spare a moment?"

Break into sentences and generate in parallel:
1. "Well, uh, I don't know if you could help me out."
2. "I was wondering if you could possibly spare a moment?"

### Implementation Strategy

**Backend (server.py) Changes:**

```python
# Current (SLOW):
audio = await generate_tts_audio(full_text, agent_config)
play_audio(audio)

# New (FAST):
sentences = split_into_sentences(full_text)
for sentence in sentences:
    audio = await generate_tts_audio(sentence, agent_config)  # 2-4s each
    play_audio(audio)  # Play while next sentence generates
```

### Benefits:
- First sentence plays in 2-4s (not 30-60s)
- Subsequent sentences overlap generation with playback
- Total perceived latency: 2-4s (first audio)
- User hears response immediately

### Latency Breakdown:

**Current (waiting for full text):**
```
Generate "Well uh... spare a moment?" â†’ 30s
Play entire audio â†’ 30s + playback time
Total: 35s before user hears anything
```

**Sentence-level (streaming):**
```
Generate sentence 1 â†’ 2s â†’ Play (5s) â† User hears!
  â†“ (parallel)
Generate sentence 2 â†’ 3s â†’ Play (4s)
Total: 2s before user hears first audio
```

### Implementation:

```python
import re

def split_into_sentences(text: str) -> list:
    """Split text into sentences for streaming"""
    # Basic sentence splitting
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # Combine very short sentences (< 10 chars)
    combined = []
    buffer = ""
    
    for sent in sentences:
        if len(sent) < 10 and buffer:
            buffer += " " + sent
        else:
            if buffer:
                combined.append(buffer)
            buffer = sent
    
    if buffer:
        combined.append(buffer)
    
    return combined


async def stream_tts_sentence_by_sentence(text: str, agent_config: dict):
    """Generate and play TTS sentence by sentence"""
    sentences = split_into_sentences(text)
    
    logger.info(f"ğŸ“ Streaming {len(sentences)} sentences")
    
    tasks = []
    for i, sentence in enumerate(sentences):
        # Generate sentence
        audio_bytes = await generate_tts_audio(sentence, agent_config)
        
        # Play immediately (don't wait for next sentence)
        task = asyncio.create_task(play_sentence(sentence, audio_bytes, i))
        tasks.append(task)
        
        logger.info(f"âš¡ Sentence {i+1}/{len(sentences)} playing while generating next...")
    
    # Wait for all to complete
    await asyncio.gather(*tasks)
```

### Real Performance Example:

**Text**: "Well, uh, I don't know if you could help me out. I was wondering if you could possibly spare a moment?"

**Current**: 35+ seconds

**Sentence-level**:
- t=0s: Start generating sentence 1
- t=2s: Play sentence 1, start generating sentence 2
- t=7s: Sentence 1 done, sentence 2 playing
- Total: 2s to first audio, 12s total (60% faster)

---

## RunPod Code is FINE

The RunPod code doesn't need to change much. The bottleneck is the MODEL itself - it CAN'T stream iteratively during generation. 

The solution is sending SHORTER text inputs (sentences) instead of long paragraphs.

---

## Implementation Priority:

1. âœ… **Immediate**: Implement sentence-level chunking in server.py (20 min)
2. â³ Later: Optimize RunPod GPU settings
3. â³ Advanced: Implement audio caching for common phrases

This will give you 2-4s latency comparable to ElevenLabs!
