# TTS Streaming Latency Investigation Report

## Executive Summary

This document chronicles all attempts to reduce intra-node TTS latency in the voice agent system. The core problem is that audio playback waits for entire sentences to be generated before playing, resulting in 2-5 second delays. Multiple fix attempts were made, all resulting in regressions to critical functionality.

**Date:** December 13, 2025  
**Status:** UNRESOLVED - Reverted to stable code  
**Priority:** P2 (Important but not blocking)

---

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [System Architecture Overview](#system-architecture-overview)
3. [Root Cause Analysis](#root-cause-analysis)
4. [Attempt #1: Batch Audio Chunks by Time](#attempt-1-batch-audio-chunks-by-time)
5. [Attempt #2: audioop Direct Conversion](#attempt-2-audioop-direct-conversion)
6. [Comprehensive Risk Analysis](#comprehensive-risk-analysis)
7. [Interconnected Systems Map](#interconnected-systems-map)
8. [Future Considerations](#future-considerations)
9. [Appendix: Code References](#appendix-code-references)

---

## Problem Statement

### The Symptom
When the agent responds within a node (intra-node responses like Q&A answers, reprompts for missing variables), there is significantly higher latency compared to simple node transitions.

**Observed Latency:**
- Transition evaluations: ~350-650ms (fast)
- Intra-node response generation: 2,000-12,000ms (slow)

### Why Transitions Are Fast
Transition evaluations are quick LLM calls with:
- Small prompt (just condition matching)
- `max_tokens=10` (only returning a number)
- `temperature=0` (deterministic, faster)
- No TTS generation required

### Why Intra-Node Responses Are Slow

The response generation within a node involves:

1. **Heavy LLM Prompt Processing**
   - Full cached system prompt (~4159 chars global + 866 chars technical)
   - Knowledge Base injection (when needs_kb=true)
   - Session variables context
   - Full conversation history
   - Node-specific instructions

2. **KB/RAG Retrieval** - ChromaDB/MongoDB retrieval time before LLM call

3. **Mandatory Variable Extraction & Reprompts** - Each missing variable triggers another full LLM call

4. **TTS Pipeline is Sequential** - After LLM response:
   - ElevenLabs WebSocket connection
   - Audio generation per sentence
   - **WAIT FOR ALL CHUNKS** ← THE BOTTLENECK
   - Buffer throttling
   - Telnyx playback commands

### The Core Bottleneck

In `/app/backend/persistent_tts_service.py`, the `stream_sentence()` method:

```python
# CURRENT BEHAVIOR (PROBLEMATIC):
audio_chunks = []
async for audio_chunk in self.ws_service.receive_audio_chunks():
    audio_chunks.append(audio_chunk)  # Collect ALL chunks

# Only AFTER all chunks received:
full_audio = b''.join(audio_chunks)
await self.audio_queue.put({...})  # NOW queue for playback
```

This defeats the purpose of streaming - audio doesn't start playing until the ENTIRE sentence is generated by ElevenLabs (2-5 seconds).

---

## System Architecture Overview

### Audio Flow Pipeline

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           AUDIO GENERATION FLOW                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  1. LLM generates text                                                       │
│         ↓                                                                    │
│  2. Text split into sentences                                                │
│         ↓                                                                    │
│  3. stream_callback() called per sentence                                    │
│         ↓                                                                    │
│  4. stream_sentence_to_tts() in server.py                                   │
│         ↓                                                                    │
│  5. persistent_tts_session.stream_sentence()                                │
│         ↓                                                                    │
│  6. ElevenLabs WebSocket generates audio chunks                             │
│         ↓                                                                    │
│  7. ⚠️ BOTTLENECK: All chunks collected into list                           │
│         ↓                                                                    │
│  8. Chunks joined → queued to audio_queue                                   │
│         ↓                                                                    │
│  9. _playback_consumer() reads from queue                                   │
│         ↓                                                                    │
│  10. _play_audio_chunk() processes audio                                    │
│         ↓                                                                    │
│  11. PCM → MP3 (FFmpeg) → mulaw (FFmpeg) → WebSocket                       │
│         ↓                                                                    │
│  12. Telnyx plays audio on phone                                            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Key State Variables

| Variable | Location | Purpose |
|----------|----------|---------|
| `is_speaking` | PersistentTTSSession | Tracks if agent is currently speaking |
| `interrupted` | PersistentTTSSession | Flag set when user interrupts |
| `playback_expected_end_time` | call_states dict | When current audio should finish |
| `_stream_lock` | PersistentTTSSession | Ensures sentences processed in order |

### Critical Timing Dependencies

```
┌────────────────────────────────────────────────────────────────────────┐
│                    INTERRUPTION HANDLING FLOW                          │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  User speaks while agent is talking                                    │
│         ↓                                                              │
│  Soniox detects speech → handle_user_speech() in server.py            │
│         ↓                                                              │
│  Check: is_speaking == True?                                          │
│         ↓ YES                                                          │
│  Set: interrupted = True                                               │
│         ↓                                                              │
│  Call: stop_and_clear()                                                │
│         ↓                                                              │
│  Clear audio_queue                                                     │
│         ↓                                                              │
│  _send_audio_via_websocket checks interrupted flag in send loop       │
│         ↓                                                              │
│  Stop sending, set is_speaking = False                                │
│         ↓                                                              │
│  User's speech is DISCARDED (not used for transition)                 │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

---

## Root Cause Analysis

### Why The Bottleneck Exists

The original code was likely written this way for simplicity and reliability:
1. Collect all audio data
2. Know exact duration for dead air detection
3. Single queue item per sentence = easier to manage
4. Simpler interruption handling (clear queue = stop all audio)

### What We Want To Achieve

Stream audio in smaller batches (~200ms each) so:
- First audio plays after ~200ms instead of 2-5 seconds
- User hears response start almost immediately
- Perceived latency drops dramatically

### Why This Is Hard

The change touches multiple interconnected systems:
1. **Duration tracking** - Need to know audio duration for dead air detection
2. **Interruption handling** - Need to stop mid-stream and discard buffered audio
3. **State management** - `is_speaking` flag must be accurate
4. **Audio format conversion** - PCM → MP3 → mulaw pipeline
5. **Telnyx buffering** - External system we don't fully control

---

## Attempt #1: Batch Audio Chunks by Time

### What Was Tried

Modified `stream_sentence()` in `persistent_tts_service.py` to:
1. Collect audio chunks into a buffer
2. When buffer reaches ~200ms worth of audio (6,400 bytes at 16kHz 16-bit), queue it
3. Continue collecting next batch
4. Queue final partial batch when ElevenLabs stream ends

### Code Changes

```python
# NEW APPROACH (ATTEMPTED):
BATCH_SIZE_BYTES = 6400  # ~200ms of audio at 16kHz 16-bit mono

audio_buffer = bytearray()
batch_num = 0

async for audio_chunk in self.ws_service.receive_audio_chunks():
    if self.interrupted:
        break  # Stop if user interrupted
    
    audio_buffer.extend(audio_chunk)
    
    # When we have enough for a batch, queue it immediately
    while len(audio_buffer) >= BATCH_SIZE_BYTES:
        batch_num += 1
        batch_audio = bytes(audio_buffer[:BATCH_SIZE_BYTES])
        audio_buffer = audio_buffer[BATCH_SIZE_BYTES:]
        
        await self.audio_queue.put({
            'sentence': sentence,
            'audio_pcm': batch_audio,
            'sentence_num': sentence_num,
            'is_first': is_first and batch_num == 1,
            'is_last': False
        })

# Queue remaining audio
if len(audio_buffer) > 0:
    await self.audio_queue.put({...final batch...})
```

### Result: FAILED - Audio Gaps

**Symptom:** Agent speech had gaps/stutters:
> "I - kno - w - th - is - cal - l is - out of the b - lue bu -"

**Root Cause:** Each batch went through independent MP3 encoding:
1. Batch 1: PCM → MP3 (adds MP3 headers/padding)
2. Batch 2: PCM → MP3 (adds MP3 headers/padding)
3. ...

MP3 is not a "raw" format - it has frame structure, headers, and padding. When played back-to-back, there are audible gaps at the boundaries between independently-encoded MP3 segments.

### Why MP3 Batching Doesn't Work

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     MP3 FRAME STRUCTURE                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Single MP3 File:                                                        │
│  [HEADER][FRAME1][FRAME2][FRAME3]...[PADDING]                           │
│                                                                          │
│  Multiple Small MP3 Files Concatenated:                                  │
│  [HEADER][F1][F2][PAD] + [HEADER][F3][F4][PAD] + [HEADER][F5][F6][PAD]  │
│           ↑                      ↑                      ↑                │
│           └── GAP ───────────────┴── GAP ───────────────┘                │
│                                                                          │
│  Each MP3 file has its own header and padding, creating audible gaps    │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Lessons Learned

1. Cannot simply batch audio and convert each batch independently to MP3
2. Need to either:
   - Use a raw format that concatenates cleanly (mulaw, PCM)
   - Or stream through a single MP3 encoder session
3. The conversion pipeline (PCM → MP3 → mulaw) was designed for complete sentences

---

## Attempt #2: audioop Direct Conversion

### What Was Tried

Bypass FFmpeg entirely using Python's built-in `audioop` module:
1. Use `audioop.ratecv()` to downsample PCM from 16kHz to 8kHz
2. Use `audioop.lin2ulaw()` to convert linear PCM to mu-law encoding
3. Send mulaw directly to Telnyx WebSocket (no MP3 intermediate step)

### Theory

Mulaw is a raw audio format - just encoded samples with no headers or framing. Multiple mulaw segments should concatenate cleanly without gaps.

### Code Changes

```python
import audioop

# In _play_audio_chunk():
if self.telnyx_ws:
    # FAST PATH: Use audioop for direct PCM→mulaw conversion
    # Step 1: Downsample PCM from 16kHz to 8kHz
    pcm_8khz, _ = audioop.ratecv(audio_pcm, 2, 1, 16000, 8000, None)
    
    # Step 2: Convert linear PCM to mu-law encoding
    mulaw_data = audioop.lin2ulaw(pcm_8khz, 2)
    
    # Send directly via WebSocket
    success = await self._send_mulaw_via_websocket(mulaw_data)
```

Also created new method `_send_mulaw_via_websocket()` that:
- Sends mulaw chunks to Telnyx
- Sets `is_speaking = True`
- Updates `playback_expected_end_time`
- Checks `interrupted` flag during send loop
- Schedules `reset_speaking_after_playback` task

### Result: FAILED - Broke Interruption Handling

**Symptom:** When user said "banana" to interrupt:
1. Agent did NOT stop speaking (interruption didn't work)
2. After agent finished, "banana" was used to trigger a transition
3. This is a critical regression - user's affirmative sounds ("yeah", "uh-huh") while listening would cause unwanted transitions

**Root Cause Analysis:**

The interruption didn't trigger because something in the timing of `is_speaking` flag was different. The exact failure mode was not fully diagnosed before revert was required.

Potential causes:
1. **Timing difference:** audioop conversion is nearly instant (~1ms) vs FFmpeg (~100-200ms). The `is_speaking` flag was set at a different point in the pipeline.
2. **Race condition:** The faster path might have completed sending before the interruption check could happen.
3. **State synchronization:** The new `_send_mulaw_via_websocket` method might not have been setting state correctly relative to when user speech was detected.

### Why This Regression Is Critical

```
┌─────────────────────────────────────────────────────────────────────────┐
│              INTERRUPTION REGRESSION SCENARIO                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  EXPECTED BEHAVIOR:                                                      │
│  Agent: "So I wanted to talk to you about..."                           │
│  User: "yeah" (affirmative while listening)                             │
│  Agent: [continues speaking, ignores "yeah"]                            │
│  Agent: "...the opportunity we have."                                   │
│  [Agent waits for user response]                                        │
│                                                                          │
│  BROKEN BEHAVIOR:                                                        │
│  Agent: "So I wanted to talk to you about..."                           │
│  User: "yeah" (affirmative while listening)                             │
│  Agent: [finishes current speech]                                       │
│  Agent: [uses "yeah" as input, transitions to next node]                │
│  User: [confused, never got to respond]                                 │
│                                                                          │
│  This breaks natural conversation flow!                                  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Lessons Learned

1. The `is_speaking` flag timing is CRITICAL
2. Changes to the audio pipeline affect interruption handling in non-obvious ways
3. The original FFmpeg pipeline, while slower, has timing that works with interruption detection
4. Need to understand exactly when `is_speaking` is checked in the user speech detection flow

---

## Comprehensive Risk Analysis

### Risk #1: Interruption Handling

**What It Does:**
When user speaks while agent is talking, the agent should:
1. Stop speaking immediately
2. Discard the user's interrupting utterance (not use it for transition)
3. Wait for user's actual response

**How It Works:**
1. Soniox detects user speech
2. `handle_user_speech()` in server.py checks `is_speaking` flag
3. If `is_speaking == True`:
   - Set `interrupted = True`
   - Call `stop_and_clear()` to clear audio queue
   - Discard the transcript
4. `_send_audio_via_websocket` checks `interrupted` flag in send loop
5. If interrupted, stop sending and set `is_speaking = False`

**What Can Break It:**
- `is_speaking` flag not set at right time
- `interrupted` flag not checked in send loop
- Race condition between flag setting and checking
- Timing changes due to faster/slower audio processing

**How To Test:**
1. Start a call
2. Let agent start speaking a long sentence
3. Say "banana" in the middle
4. Agent should stop immediately
5. "banana" should NOT trigger a transition
6. Agent should wait for actual user response

### Risk #2: Dead Air Detection

**What It Does:**
Detects when the agent should be speaking but audio has stopped unexpectedly.

**How It Works:**
1. `playback_expected_end_time` is set when audio is queued
2. Background task checks if current time > expected end time
3. If audio should have finished but hasn't, trigger dead air handling

**What Can Break It:**
- Duration calculation incorrect (wrong sample rate, wrong formula)
- `playback_expected_end_time` not extended for multiple batches
- Each batch overwriting instead of extending the time

**Duration Calculation:**
```python
# Mulaw at 8kHz = 8000 bytes per second
actual_duration_seconds = len(mulaw_data) / 8000.0
```

If using batches, must EXTEND not OVERWRITE:
```python
base_time = max(current_expected_end, current_time)
new_expected_end = base_time + actual_duration_seconds + buffer
```

### Risk #3: Audio Jumbling/Order Issues

**What It Does:**
Ensures audio plays in correct order - sentence 1 before sentence 2, etc.

**How It Works:**
1. `_stream_lock` ensures only one sentence processed at a time
2. Queue is FIFO - items processed in order added
3. `sentence_num` tracks ordering for logging

**What Can Break It:**
- Removing or bypassing `_stream_lock`
- Async operations completing out of order
- Multiple batches from different sentences interleaving

**Why It's Safe With Batching:**
The `_stream_lock` ensures sentence 1 is fully queued before sentence 2 starts. Within a sentence, batches are queued sequentially. Queue is FIFO, so order is preserved.

### Risk #4: Audio Artifacts/Quality Issues

**What It Does:**
Ensures audio sounds natural without clicks, pops, gaps, or distortion.

**Potential Issues:**
1. **MP3 headers** - Each MP3 file has headers that cause gaps when concatenated
2. **Sample rate mismatch** - If input/output rates don't match, audio sounds wrong
3. **Bit depth issues** - Wrong width parameter in audioop causes distortion
4. **Encoding errors** - audioop might handle edge cases differently than FFmpeg

**Current Pipeline:**
```
ElevenLabs (PCM 16kHz 16-bit) 
    → FFmpeg (PCM → MP3) 
    → FFmpeg (MP3 → mulaw 8kHz) 
    → Telnyx WebSocket
```

**Proposed Pipeline:**
```
ElevenLabs (PCM 16kHz 16-bit) 
    → audioop.ratecv (16kHz → 8kHz) 
    → audioop.lin2ulaw (PCM → mulaw) 
    → Telnyx WebSocket
```

### Risk #5: 1-Word Responses

**What It Does:**
Handles short user responses like "yes", "no", "okay".

**Potential Issues:**
- Very short audio might be less than one batch size
- Edge case handling for small buffers
- Timing might be different for short vs long responses

**Why It Should Be Safe:**
Short responses just create a small final batch. The logic handles partial batches at the end.

### Risk #6: Buffer Throttling

**What It Does:**
Prevents building up too much buffered audio at Telnyx.

**How It Works:**
```python
MAX_BUFFER_AHEAD = 2.0  # seconds
if buffer_ahead > MAX_BUFFER_AHEAD:
    wait_time = buffer_ahead - MAX_BUFFER_AHEAD
    # Wait, checking interrupted flag
```

**What Can Break It:**
- Many small batches could overwhelm the throttling logic
- Wait time calculations might be wrong with batched audio
- More frequent checks of interrupted flag (actually good)

### Risk #7: REST API Fallback

**What It Does:**
Falls back to REST API if WebSocket not available.

**Current Implementation:**
Uses MP3 files uploaded to a URL, then Telnyx REST API to play.

**What Can Break It:**
If MP3 creation is skipped for WebSocket path, REST fallback won't have MP3 files.

**Solution:**
Keep REST path unchanged. Only optimize WebSocket path.

---

## Interconnected Systems Map

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        SYSTEM INTERCONNECTIONS                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐                                                        │
│  │   server.py     │                                                        │
│  │                 │                                                        │
│  │ handle_user_    │◄─── Soniox speech detection                           │
│  │ speech()        │                                                        │
│  │      │          │                                                        │
│  │      ▼          │                                                        │
│  │ checks          │                                                        │
│  │ is_speaking ────┼──────────────────────┐                                │
│  │      │          │                      │                                │
│  │      ▼          │                      │                                │
│  │ sets            │                      │                                │
│  │ interrupted ────┼───────────┐          │                                │
│  │                 │           │          │                                │
│  └─────────────────┘           │          │                                │
│                                │          │                                │
│  ┌─────────────────────────────┼──────────┼─────────────────────────────┐  │
│  │ persistent_tts_service.py   │          │                             │  │
│  │                             │          │                             │  │
│  │  stream_sentence()          │          │                             │  │
│  │       │                     │          │                             │  │
│  │       ▼                     │          │                             │  │
│  │  [TTS chunks] ──────────────┼──────────┼────────┐                    │  │
│  │       │                     │          │        │                    │  │
│  │       ▼                     │          │        │                    │  │
│  │  audio_queue ───────────────┼──────────┼────────┼───┐                │  │
│  │       │                     │          │        │   │                │  │
│  │       ▼                     │          │        │   │                │  │
│  │  _playback_consumer()       │          │        │   │                │  │
│  │       │                     │          │        │   │                │  │
│  │       ▼                     │          │        │   │                │  │
│  │  _play_audio_chunk()        │          │        │   │                │  │
│  │       │                     │          │        │   │                │  │
│  │       ▼                     ▼          ▼        │   │                │  │
│  │  _send_audio_via_websocket()                    │   │                │  │
│  │       │                                         │   │                │  │
│  │       ├─► checks interrupted ◄──────────────────┘   │                │  │
│  │       │                                             │                │  │
│  │       ├─► sets is_speaking ◄────────────────────────┘                │  │
│  │       │                                                              │  │
│  │       ├─► updates playback_expected_end_time                         │  │
│  │       │          │                                                   │  │
│  │       │          ▼                                                   │  │
│  │       │   call_states dict (in server.py)                           │  │
│  │       │          │                                                   │  │
│  │       │          ▼                                                   │  │
│  │       │   dead air detection                                         │  │
│  │       │                                                              │  │
│  │       ▼                                                              │  │
│  │  Telnyx WebSocket ──► Phone                                         │  │
│  │                                                                      │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Future Considerations

### Approach A: Stream Raw Mulaw Through Single Pipeline

Instead of batching and converting each batch, stream through a single conversion pipeline:

1. Start FFmpeg as a persistent subprocess
2. Pipe PCM chunks in as they arrive
3. Read mulaw chunks out as they're produced
4. Send to Telnyx immediately

**Pros:**
- Single encoding session = no gaps
- True streaming

**Cons:**
- Complex subprocess management
- Need to handle FFmpeg lifecycle
- Potential for stuck processes

### Approach B: Use ElevenLabs Mulaw Output Directly

ElevenLabs might support mulaw output format directly:

```python
output_format: str = "ulaw_8000"  # Check if supported
```

**Pros:**
- No conversion needed
- True streaming from source

**Cons:**
- May not be supported
- May have different quality characteristics

### Approach C: Optimize FFmpeg Calls

Instead of file I/O, use FFmpeg with pipes:

```python
# Instead of:
# PCM → file → FFmpeg → file → read

# Use:
# PCM → pipe → FFmpeg → pipe → send
process = subprocess.Popen(
    ['ffmpeg', '-f', 's16le', '-ar', '16000', '-ac', '1', '-i', 'pipe:0',
     '-ar', '8000', '-f', 'mulaw', 'pipe:1'],
    stdin=subprocess.PIPE,
    stdout=subprocess.PIPE
)
mulaw_data = process.communicate(input=audio_pcm)[0]
```

**Pros:**
- No file I/O
- ~50-100ms savings per sentence

**Cons:**
- Still not streaming (waits for complete input)
- Doesn't solve the batching problem

### Approach D: Accept Latency, Optimize Elsewhere

Focus optimization efforts on:
1. Reduce LLM prompt size
2. Use faster LLM model
3. Optimize KB retrieval
4. Reduce mandatory variable checks

**Pros:**
- No changes to fragile audio pipeline
- Might get similar latency reduction

**Cons:**
- Doesn't address fundamental TTS bottleneck
- May not be enough improvement

### Testing Requirements for Any Future Attempt

Before deploying any fix, must test:

1. **Basic playback** - Audio plays clearly without artifacts
2. **Interruption** - User can interrupt mid-sentence
3. **Interruption discard** - Interrupting utterance is not used for transition
4. **Dead air detection** - Still triggers correctly
5. **Short responses** - 1-word responses work
6. **Long responses** - Multi-sentence responses work
7. **Multi-turn conversation** - Multiple back-and-forth exchanges
8. **Rapid interruption** - User interrupts multiple times quickly
9. **Edge cases** - Empty responses, very long responses, special characters

---

## Appendix: Code References

### Key Files

| File | Purpose |
|------|---------|
| `/app/backend/persistent_tts_service.py` | TTS streaming, audio queue, playback |
| `/app/backend/server.py` | Webhook handling, interruption detection |
| `/app/backend/elevenlabs_ws_service.py` | ElevenLabs WebSocket client |
| `/app/backend/calling_service.py` | Call flow, node processing |

### Key Functions

| Function | File | Purpose |
|----------|------|---------|
| `stream_sentence()` | persistent_tts_service.py | Sends text to TTS, collects audio |
| `_playback_consumer()` | persistent_tts_service.py | Reads from queue, plays audio |
| `_play_audio_chunk()` | persistent_tts_service.py | Converts and sends audio |
| `_send_audio_via_websocket()` | persistent_tts_service.py | Sends mulaw to Telnyx |
| `handle_user_speech()` | server.py | Processes user speech, handles interruption |
| `stop_and_clear()` | persistent_tts_service.py | Clears queue on interruption |

### Key State Variables

| Variable | Type | Location | Purpose |
|----------|------|----------|---------|
| `is_speaking` | bool | PersistentTTSSession | True when agent is speaking |
| `interrupted` | bool | PersistentTTSSession | True when user interrupted |
| `playback_expected_end_time` | float | call_states dict | When audio should finish |
| `_stream_lock` | asyncio.Lock | PersistentTTSSession | Ensures sentence ordering |
| `audio_queue` | asyncio.Queue | PersistentTTSSession | Queued audio for playback |

### Audio Format Specifications

| Stage | Format | Sample Rate | Bit Depth | Channels |
|-------|--------|-------------|-----------|----------|
| ElevenLabs output | PCM | 16 kHz | 16-bit signed LE | Mono |
| FFmpeg intermediate | MP3 | 16 kHz | 64 kbps | Mono |
| Telnyx input | mulaw | 8 kHz | 8-bit | Mono |

### Timing Constants

| Constant | Value | Purpose |
|----------|-------|---------|
| `MAX_BUFFER_AHEAD` | 2.0 seconds | Max audio buffered at Telnyx |
| `chunk_size` | 160 bytes | 20ms of 8kHz mulaw |
| `BATCH_SIZE_BYTES` | 6400 bytes | ~200ms of 16kHz 16-bit PCM (attempted) |

---

## Conclusion

The TTS streaming latency issue remains unresolved. Two approaches were attempted:

1. **Batching audio chunks** - Failed due to MP3 header gaps
2. **audioop direct conversion** - Failed due to breaking interruption handling

The fundamental challenge is that the audio pipeline is deeply integrated with state management for interruption handling. Changes that seem isolated to audio conversion actually affect timing-sensitive state checks.

Future attempts should:
1. Thoroughly understand the `is_speaking` and `interrupted` flag timing
2. Test interruption handling extensively before any other testing
3. Consider approaches that don't change the conversion pipeline timing
4. Have sufficient credits/time for multiple test iterations

The current stable code should be preserved until a more comprehensive solution can be developed and tested.
