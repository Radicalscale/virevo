# Comprehensive AI Call Testing Guide

## ğŸ¯ Purpose
This guide helps you verify that your AI calling agent is properly loading and using:
- âœ… Correct STT provider (Soniox, Deepgram, AssemblyAI, etc.)
- âœ… Correct TTS provider (ElevenLabs, Cartesia, etc.)
- âœ… Correct LLM provider and model
- âœ… Agent tools and knowledge base
- âœ… Interruption handling
- âœ… Low latency performance

---

## ğŸ“‹ Pre-Test Checklist

### Environment Variables (Railway):
- [ ] `BACKEND_URL` = Your Railway domain
- [ ] `ENCRYPTION_KEY` = Valid Fernet key (44 chars)
- [ ] `MONGO_URL` = MongoDB connection string
- [ ] `REDIS_URL` = Redis internal URL
- [ ] `JWT_SECRET_KEY` = Random 32-char string
- [ ] `CORS_ORIGINS` = Your frontend URL
- [ ] `TELNYX_API_KEY` = Your Telnyx key
- [ ] Provider keys (Soniox, ElevenLabs, etc.)

### Telnyx Configuration:
- [ ] Webhook URL set to: `https://your-backend/api/webhook/telnyx`
- [ ] Phone number purchased and connected
- [ ] Connection active

### Agent Configuration (Frontend):
- [ ] Agent created with name
- [ ] STT provider selected (e.g., Soniox)
- [ ] TTS provider selected (e.g., ElevenLabs)
- [ ] LLM configured
- [ ] Phone number assigned
- [ ] Agent prompt configured
- [ ] API keys saved in Settings

---

## ğŸ§ª Test 1: Basic Call Connection

### Objective:
Verify call connects and webhook is received.

### Steps:
1. Make outbound call from frontend
2. Answer the phone when it rings
3. Monitor Railway logs

### Expected Logs:
```
âœ… ğŸ“ Outbound call initiated: v3:xxxxx
âœ… ğŸ“¦ Call data stored in Redis
âœ… ğŸ¯ Webhook received: call.answered for v3:xxxxx
```

### Success Criteria:
- [ ] Call connects and rings
- [ ] Phone answers
- [ ] Backend receives `call.answered` webhook
- [ ] No errors in logs

### If Failed:
- Check BACKEND_URL is set correctly
- Verify Telnyx webhook URL in portal
- Check TELNYX_API_KEY is valid
- Look for errors in logs

---

## ğŸ§ª Test 2: Agent Configuration Loading

### Objective:
Verify agent settings are loaded from database.

### Steps:
1. Before calling, note your agent's configuration in UI
2. Make a call
3. Check Railway logs for agent initialization

### Expected Logs:
```
âœ… ğŸ”§ Loading agent configuration: [agent_id]
âœ… ğŸ“‹ Agent: [agent_name]
âœ… ğŸ¤ STT Provider: Soniox
âœ… ğŸ”Š TTS Provider: ElevenLabs
âœ… ğŸ§  LLM: openai/gpt-4
âœ… ğŸ“š Knowledge Base: enabled
âœ… ğŸ› ï¸ Tools: [tool1, tool2]
âœ… âš¡ Interruption: enabled
```

### Success Criteria:
- [ ] Agent settings match UI configuration
- [ ] Correct STT provider loaded
- [ ] Correct TTS provider loaded
- [ ] Correct LLM loaded
- [ ] Tools and KB settings correct

### If Failed:
- Check agent ID is correct
- Verify agent exists in database
- Check MONGO_URL connection
- Look for MongoDB connection errors

---

## ğŸ§ª Test 3: API Key Decryption

### Objective:
Verify API keys are properly encrypted and decrypted.

### Steps:
1. Make a call
2. Check logs for decryption

### Expected Logs:
```
âœ… ğŸ”‘ Decrypting API keys for agent
âœ… ğŸ”‘ Soniox API key loaded
âœ… ğŸ”‘ ElevenLabs API key loaded
âœ… ğŸ”‘ OpenAI API key loaded
```

### Should NOT See:
```
âŒ Failed to decrypt key, assuming unencrypted
âŒ Fernet key must be 32 url-safe base64-encoded bytes
```

### Success Criteria:
- [ ] No decryption errors
- [ ] All provider keys loaded
- [ ] Keys work with services

### If Failed:
- Regenerate ENCRYPTION_KEY (use `/app/generate_encryption_key.py`)
- Re-enter all API keys in frontend
- Restart backend

---

## ğŸ§ª Test 4: STT (Soniox) Initialization

### Objective:
Verify Soniox STT service initializes and receives audio.

### Steps:
1. Make a call
2. Say "Hello, can you hear me?"
3. Check logs

### Expected Logs:
```
âœ… ğŸ¤ Initializing Soniox STT service
âœ… ğŸ”‘ Soniox API key: KEY_xxxx (first 8 chars)
âœ… ğŸŒ Connecting to Soniox WebSocket
âœ… âœ… Soniox STT ready
âœ… ğŸ™ï¸ Audio streaming started
âœ… ğŸ“ Audio chunk sent to Soniox
âœ… ğŸ“ Transcription: "Hello, can you hear me?"
```

### Success Criteria:
- [ ] Soniox initializes without errors
- [ ] WebSocket connection established
- [ ] Audio is transcribed
- [ ] Transcription is accurate
- [ ] Latency < 1 second

### If Failed:
- Check SONIOX_API_KEY is valid
- Verify Soniox account has credits
- Check WebSocket connection logs
- Test Soniox API key directly

---

## ğŸ§ª Test 5: LLM Response Generation

### Objective:
Verify LLM processes transcription and generates response.

### Steps:
1. During call, say something
2. Check logs for LLM activity

### Expected Logs:
```
âœ… ğŸ“ Transcription: "What's the weather like?"
âœ… ğŸ§  Sending to LLM: openai/gpt-4
âœ… ğŸ¤– LLM Response: "I don't have access to weather..."
```

### Success Criteria:
- [ ] LLM receives transcription
- [ ] LLM generates response
- [ ] Response is contextually relevant
- [ ] Response follows agent prompt

### If Failed:
- Check LLM API key is valid
- Verify LLM model is available
- Check prompt configuration
- Look for LLM API errors

---

## ğŸ§ª Test 6: TTS (ElevenLabs) Synthesis

### Objective:
Verify TTS synthesizes and streams audio.

### Steps:
1. Wait for agent to respond
2. Listen for audio
3. Check logs

### Expected Logs:
```
âœ… ğŸ”Š Sending to TTS: ElevenLabs
âœ… ğŸµ TTS generated: 3.2 seconds of audio
âœ… ğŸ“¡ Streaming audio to caller
âœ… âœ… Audio sent successfully
```

### Success Criteria:
- [ ] TTS generates audio
- [ ] Audio is streamed to caller
- [ ] Voice quality is good
- [ ] Latency is acceptable
- [ ] No audio glitches

### If Failed:
- Check ElevenLabs API key
- Verify voice ID exists
- Check audio format compatibility
- Look for TTS errors

---

## ğŸ§ª Test 7: Knowledge Base Integration

### Objective:
Verify KB is loaded and used in responses.

### Steps:
1. Add knowledge to agent's KB
2. Make a call
3. Ask a question about the KB content
4. Check logs

### Expected Logs:
```
âœ… ğŸ“š Loading knowledge base for agent
âœ… ğŸ“Š KB loaded: 15 documents
âœ… ğŸ” Searching KB for: "product pricing"
âœ… ğŸ“„ KB results found: 3 relevant chunks
âœ… ğŸ¤– Using KB context in LLM prompt
```

### Success Criteria:
- [ ] KB loads successfully
- [ ] KB search works
- [ ] Relevant content retrieved
- [ ] Agent uses KB in response
- [ ] Response includes KB information

### If Failed:
- Check KB documents are uploaded
- Verify RAG service is enabled
- Check vector embeddings exist
- Look for ChromaDB errors

---

## ğŸ§ª Test 8: Tool Execution

### Objective:
Verify agent tools execute correctly.

### Steps:
1. Configure agent with tools (e.g., "book_appointment")
2. Make a call
3. Trigger tool usage
4. Check logs

### Expected Logs:
```
âœ… ğŸ› ï¸ Agent tools loaded: [book_appointment, send_email]
âœ… ğŸ¤– LLM decides to use tool: book_appointment
âœ… ğŸ”§ Executing tool: book_appointment
âœ… ğŸ“Š Tool parameters: {"date": "2025-11-15", "time": "10:00"}
âœ… âœ… Tool executed successfully
âœ… ğŸ¤– LLM response includes tool result
```

### Success Criteria:
- [ ] Tools are loaded
- [ ] LLM decides to use tool appropriately
- [ ] Tool executes successfully
- [ ] Tool result is used in response
- [ ] User is informed of action

### If Failed:
- Check tool configuration in agent
- Verify tool function definitions
- Check tool execution permissions
- Look for tool execution errors

---

## ğŸ§ª Test 9: Interruption Handling

### Objective:
Verify agent stops when user interrupts.

### Steps:
1. Make a call
2. Let agent start speaking (long response)
3. Interrupt mid-sentence by speaking
4. Check logs

### Expected Logs:
```
âœ… ğŸ”Š TTS streaming to caller
âœ… ğŸ™ï¸ User speech detected during TTS
âœ… âš¡ Interruption detected
âœ… ğŸ›‘ Stopping TTS playback
âœ… ğŸ¤ Listening to user...
âœ… ğŸ“ Transcription: "Wait, let me ask..."
```

### Success Criteria:
- [ ] Agent stops speaking immediately
- [ ] User speech is captured
- [ ] No audio overlap
- [ ] Conversation continues naturally
- [ ] No dropped audio

### If Failed:
- Check interruption settings in agent config
- Verify VAD (Voice Activity Detection) is working
- Check audio streaming configuration
- Look for WebSocket errors

---

## ğŸ§ª Test 10: Multi-Agent Configuration

### Objective:
Verify different agents use their own configurations.

### Steps:
1. Create Agent A: Deepgram STT + Cartesia TTS
2. Create Agent B: Soniox STT + ElevenLabs TTS
3. Call Agent A, note behavior
4. Call Agent B, note behavior
5. Compare logs

### Expected Behavior:
- Agent A uses Deepgram and Cartesia
- Agent B uses Soniox and ElevenLabs
- No configuration bleeding
- Each agent maintains own settings

### Success Criteria:
- [ ] Each agent uses correct STT
- [ ] Each agent uses correct TTS
- [ ] No configuration conflicts
- [ ] Consistent behavior per agent

### If Failed:
- Check agent IDs are unique
- Verify config is stored per agent
- Check Redis state isolation
- Look for configuration caching issues

---

## ğŸ§ª Test 11: Low Latency Performance

### Objective:
Measure end-to-end latency.

### Steps:
1. Make a call
2. Say "Hello"
3. Measure time until agent responds
4. Check logs for timing

### Expected Logs:
```
âœ… ğŸ“ Transcription received: 450ms
âœ… ğŸ§  LLM response: 800ms
âœ… ğŸ”Š TTS synthesis: 300ms
âœ… â±ï¸ Total latency: 1.55s
```

### Success Criteria:
- [ ] STT latency < 500ms
- [ ] LLM latency < 1s
- [ ] TTS latency < 500ms
- [ ] Total latency < 2s
- [ ] Conversation feels natural

### If Failed:
- Check network connectivity
- Verify provider APIs are responsive
- Optimize agent prompt (shorter = faster)
- Consider faster models/providers
- Check for rate limiting

---

## ğŸ§ª Test 12: Error Recovery

### Objective:
Verify graceful error handling.

### Steps:
1. Temporarily invalidate an API key
2. Make a call
3. Check error handling
4. Fix key and retry

### Expected Logs:
```
âŒ ğŸ”‘ Error: Invalid Soniox API key
âœ… âš ï¸ Falling back to error message
âœ… ğŸ”Š Playing error TTS to user
âœ… ğŸ“ Call continues with error state
```

### Success Criteria:
- [ ] Errors are caught and logged
- [ ] User is informed of issue
- [ ] Call doesn't crash
- [ ] Graceful degradation
- [ ] Recovery after fix

### If Failed:
- Add more error handling
- Improve error messages
- Add fallback mechanisms
- Better logging

---

## ğŸ“Š Complete Success Checklist

After all tests, you should have:

### âœ… Call Flow:
- [ ] Calls connect reliably
- [ ] Webhooks received
- [ ] WebSockets established
- [ ] Audio streaming works

### âœ… Agent Configuration:
- [ ] Correct STT provider loaded
- [ ] Correct TTS provider loaded
- [ ] Correct LLM loaded
- [ ] Tools and KB working
- [ ] Interruption handling works

### âœ… Performance:
- [ ] Low latency (< 2s)
- [ ] High quality audio
- [ ] Accurate transcription
- [ ] Natural conversation flow

### âœ… Reliability:
- [ ] No crashes or errors
- [ ] Consistent behavior
- [ ] Proper error handling
- [ ] Multi-agent support

---

## ğŸ†˜ Common Issues & Solutions

### Issue: "Agent doesn't respond"
**Solutions:**
1. Check ENCRYPTION_KEY is valid Fernet format
2. Verify Telnyx webhook is configured
3. Check all API keys are valid
4. Monitor logs for errors

### Issue: "Wrong STT provider used"
**Solutions:**
1. Verify agent config in UI
2. Check database has correct settings
3. Clear Redis cache
4. Restart backend

### Issue: "High latency"
**Solutions:**
1. Use faster LLM models
2. Shorten agent prompt
3. Enable streaming responses
4. Check network connectivity

### Issue: "Transcription errors"
**Solutions:**
1. Test STT API key directly
2. Check audio format compatibility
3. Verify WebSocket connection
4. Try different STT provider

### Issue: "Tools not executing"
**Solutions:**
1. Check tool definitions in agent
2. Verify LLM has access to tools
3. Check tool execution logs
4. Test tools independently

---

## ğŸ“ˆ Performance Benchmarks

### Target Latencies:
- STT (Speech-to-Text): < 500ms
- LLM (Response Generation): < 1000ms
- TTS (Text-to-Speech): < 500ms
- **Total End-to-End: < 2000ms**

### Quality Metrics:
- Transcription Accuracy: > 95%
- Response Relevance: > 90%
- Audio Quality: Clear, no distortion
- Interruption Response: < 200ms

### Reliability Targets:
- Call Connection Rate: > 99%
- Webhook Reception Rate: 100%
- Agent Response Rate: > 95%
- Error Recovery Rate: 100%

---

## ğŸ¯ Final Verification

Before considering deployment complete:

1. **Run all 12 tests** âœ…
2. **Document any issues** ğŸ“
3. **Verify fixes work** ğŸ”§
4. **Test with real users** ğŸ‘¥
5. **Monitor production logs** ğŸ“Š

**Your AI calling system is production-ready when all tests pass!** ğŸš€
