# Phase 1: Diagnostic Instrumentation - COMPLETE ‚úÖ

**Date:** November 20, 2025  
**Status:** Successfully implemented and deployed  
**Risk Level:** ZERO (logging only, no behavior changes)

---

## Changes Implemented

### 1. TTFS (Time To First Sentence) - server.py

**Location:** Line ~2514 in `stream_sentence_to_tts()` function

**Purpose:** Measure how long it takes from user stopping speech (STT endpoint) until the first LLM sentence is ready

**Code Added:**
```python
# ‚è±Ô∏è [TIMING] First sentence arrival from LLM
if len(sentence_queue) == 1:
    ttfs_ms = int((time.time() - stt_end_time) * 1000)
    logger.info(f"‚è±Ô∏è [TIMING] TTFS (Time To First Sentence): {ttfs_ms}ms")
```

**What This Measures:**
- Time from STT endpoint detection ‚Üí First sentence from LLM
- Expected: 200-800ms for streaming responses
- If >1000ms: LLM may not be streaming properly

---

### 2. TTFT_TTS (Time To First TTS Task Started) - server.py

**Location:** Line ~2551 after TTS task creation

**Purpose:** Measure when the first TTS generation task starts (right after first sentence arrives)

**Code Added:**
```python
# ‚è±Ô∏è [TIMING] First TTS task creation
if len(tts_tasks) == 1:
    ttft_tts_ms = int((time.time() - stt_end_time) * 1000)
    logger.info(f"‚è±Ô∏è [TIMING] TTFT_TTS (First TTS Task Started): {ttft_tts_ms}ms")
```

**What This Measures:**
- Time from STT endpoint ‚Üí First TTS task started
- Should be TTFS + ~10-50ms (sentence immediately sent to TTS)
- If TTFT_TTS >> TTFS: bottleneck in TTS task creation

---

### 3. Request Timing Tracking - persistent_tts_service.py

**Location:** Line ~52-55 in `__init__`, line ~116-118 in `stream_sentence()`

**Purpose:** Track when the first sentence streaming begins (baseline for TTFA)

**Code Added:**
```python
# In __init__:
# ‚è±Ô∏è Timing tracking
self.first_audio_chunk_time = None
self.request_start_time = None

# In stream_sentence():
# ‚è±Ô∏è Track first sentence start time
if sentence_num == 1 and self.request_start_time is None:
    self.request_start_time = time.time()
```

**What This Tracks:**
- Baseline timestamp for TTFA calculation
- Resets per call (not per sentence)

---

### 4. TTFA (Time To First Audio Playback) - persistent_tts_service.py

**Location:** Line ~217-221 in `_play_audio_chunk()`

**Purpose:** Measure the CRITICAL metric - when does the first audio actually start playing to user

**Code Added:**
```python
# ‚è±Ô∏è [TIMING] Track first audio chunk playback (TTFA)
if self.first_audio_chunk_time is None and self.request_start_time is not None:
    self.first_audio_chunk_time = play_start
    ttfa_ms = int((self.first_audio_chunk_time - self.request_start_time) * 1000)
    logger.info(f"‚è±Ô∏è [TIMING] TTFA (Time To First Audio Playback): {ttfa_ms}ms")
```

**What This Measures:**
- Time from first sentence streaming start ‚Üí First audio chunk ready for playback
- **THIS IS THE KEY METRIC** - user's perceived latency
- Expected: 500-1500ms if streaming is working
- If >2000ms: may be waiting for full LLM response before starting TTS

---

## New Timing Metrics

| Metric | Meaning | Expected Range | Warning Threshold |
|--------|---------|----------------|-------------------|
| **TTFS** | Time To First Sentence from LLM | 200-800ms | >1000ms |
| **TTFT_TTS** | First TTS Task Started | TTFS + 10-50ms | TTFS + 200ms |
| **TTFA** | Time To First Audio Playback | 500-1500ms | >2000ms |

---

## Expected Log Output (Example)

```
‚è±Ô∏è [TIMING] STT_END: Endpoint detected at T+8ms
‚è±Ô∏è [TIMING] STT_LATENCY: 8ms (Soniox model: stt-rt-preview-v2)
üì§ Sentence arrived from LLM: This is Jake. I was just wondering if you could...
‚è±Ô∏è [TIMING] TTFS (Time To First Sentence): 320ms        ‚Üê NEW!
‚ö° Starting REAL-TIME TTS for first sentence
üöÄ Streaming sentence #1 via persistent WebSocket
üéµ TTS task #1 started at +15ms
‚è±Ô∏è [TIMING] TTFT_TTS (First TTS Task Started): 335ms    ‚Üê NEW!
‚è±Ô∏è [TIMING] TTFA (Time To First Audio Playback): 892ms  ‚Üê NEW! KEY METRIC
‚è±Ô∏è [TIMING] PLAYBACK_START: Processing sentence #1 (12800 bytes PCM)
‚è±Ô∏è [TIMING] TELNYX_PLAY_CALL_START: Calling play_audio_url API
‚è±Ô∏è [TIMING] TELNYX_PLAY_CALL_COMPLETE: 295ms
‚è±Ô∏è [TIMING] LLM_TOTAL: 774ms
‚è±Ô∏è [TIMING] TTS_TOTAL: 913ms
‚è±Ô∏è [TIMING] E2E_TOTAL: 1687ms
```

---

## Analysis Questions These Metrics Answer

### Question 1: Is LLM streaming working?
- **Check:** TTFS value
- **If TTFS < 500ms:** ‚úÖ Streaming works! First sentence arrives quickly
- **If TTFS > 1500ms:** ‚ùå LLM may be buffering entire response

### Question 2: Is TTS starting immediately?
- **Check:** TTFT_TTS - TTFS difference
- **If < 100ms:** ‚úÖ TTS starts immediately after first sentence
- **If > 200ms:** ‚ùå Delay between sentence arrival and TTS start

### Question 3: What's the actual perceived latency?
- **Check:** TTFA value
- **If < 1500ms:** ‚úÖ User hears response within 1.5 seconds
- **If > 2000ms:** ‚ùå User experiences noticeable delay

### Question 4: Are operations parallel or sequential?
- **Compare:** TTFA vs (TTFS + TTS_TOTAL)
- **If TTFA ‚âà TTFS + TTS_TOTAL:** ‚ùå Sequential (waiting for full TTS)
- **If TTFA < TTFS + 500ms:** ‚úÖ Parallel (TTS starts while LLM continues)

---

## Testing Instructions

### Test 1: Make a Test Call
1. Call your agent
2. Have a 2-3 turn conversation
3. Download the logs

### Test 2: Search for New Metrics
```bash
grep "TTFS\|TTFT_TTS\|TTFA" logs.txt
```

### Test 3: Analyze the Results

**Example Analysis:**
```
Turn 1:
  TTFS: 320ms       ‚Üê LLM first sentence ready
  TTFT_TTS: 335ms   ‚Üê TTS task started 15ms later (good!)
  TTFA: 892ms       ‚Üê User hears audio 892ms after request start
  LLM_TOTAL: 774ms  ‚Üê Full LLM response took 774ms
  TTS_TOTAL: 913ms  ‚Üê All TTS generation took 913ms
  E2E_TOTAL: 1687ms ‚Üê Total backend processing

Analysis:
- Streaming works! First sentence at 320ms (not waiting for 774ms)
- TTS started immediately (15ms delay)
- TTFA (892ms) < LLM_TOTAL (774ms) + TTS_TOTAL (913ms)
  ‚Üí This proves parallel execution is working!
- User heard audio at 892ms, not 1687ms
- Real improvement: 892ms vs 1687ms = 795ms faster perceived latency
```

---

## Rollback Instructions

If anything goes wrong (shouldn't - these are just logs):

```bash
cd /app/backend
git diff server.py
git diff persistent_tts_service.py

# Rollback if needed:
git checkout server.py persistent_tts_service.py
sudo supervisorctl restart backend
```

---

## Next Steps (Phase 2)

1. **Make a test call** with the new instrumentation
2. **Download logs** and search for new metrics
3. **Analyze results** using the questions above
4. **Determine if Phase 3 is needed:**
   - If TTFA < 1500ms and streaming is working: ‚úÖ **NO Phase 3 needed!**
   - If TTFA > 2000ms or sequential execution detected: ‚Üí Proceed to Phase 3

---

## Files Modified

1. `/app/backend/server.py`
   - Added TTFS logging (line ~2514)
   - Added TTFT_TTS logging (line ~2551)

2. `/app/backend/persistent_tts_service.py`
   - Added timing attributes (line ~52-55)
   - Added request_start_time tracking (line ~116-118)
   - Added TTFA logging (line ~217-221)

**Total Lines Changed:** ~15 lines added (no lines removed or modified)

---

## Risk Assessment

- **Behavior Changes:** ZERO ‚úÖ
- **Breaking Risk:** ZERO ‚úÖ
- **Performance Impact:** Negligible (<1ms per call) ‚úÖ
- **Rollback Complexity:** Trivial (3 commands) ‚úÖ

---

## Success Criteria

‚úÖ **Minimum Success:** Backend restarts without errors (ACHIEVED)  
‚úÖ **Target Success:** New metrics appear in logs for next test call  
‚úÖ **Maximum Success:** Metrics reveal streaming is already working, no Phase 3 needed  

---

**Status:** Phase 1 COMPLETE - Ready for Phase 2 testing
