import asyncio
import os
import json
import time
from typing import Dict, List
from openai import AsyncOpenAI
import logging
from dotenv import load_dotenv
from pathlib import Path
import httpx

logger = logging.getLogger(__name__)

# Load environment variables
ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# Initialize API clients
DEEPGRAM_API_KEY = os.environ.get('DEEPGRAM_API_KEY')
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
ELEVEN_API_KEY = os.environ.get('ELEVEN_API_KEY')

# Initialize OpenAI client lazily
_openai_client = None
_grok_client = None

def get_openai_client():
    global _openai_client
    if _openai_client is None and OPENAI_API_KEY:
        _openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    return _openai_client

async def get_llm_client(provider: str = "openai", api_key: str = None):
    """Get LLM client based on provider (openai or grok)"""
    if provider == "grok":
        # Use OpenAI library with xAI base URL for Grok
        try:
            import openai
            
            # Get API key from parameter or environment
            grok_key = api_key or os.environ.get('GROK_API_KEY')
            if not grok_key:
                logger.error("Grok API key not found")
                return None
            
            # Create OpenAI client with xAI base URL
            client = openai.AsyncOpenAI(
                api_key=grok_key,
                base_url="https://api.x.ai/v1"
            )
            
            # Create a wrapper to match our interface
            class GrokClient:
                def __init__(self, openai_client):
                    self.client = openai_client
                    
                async def create_completion(self, messages, model, temperature, max_tokens, stream=False):
                    """Create a completion using Grok via xAI API"""
                    try:
                        response = await self.client.chat.completions.create(
                            model=model or "grok-3",
                            messages=messages,
                            temperature=temperature,
                            max_tokens=max_tokens,
                            stream=stream
                        )
                        return response
                        
                    except Exception as e:
                        logger.error(f"Error with Grok API: {e}")
                        return None
                        
            return GrokClient(client)
            
        except ImportError as e:
            logger.error(f"OpenAI library not available: {e}")
            return None
        except Exception as e:
            logger.error(f"Error creating Grok client: {e}")
            return None
    else:
        # Default to OpenAI
        return get_openai_client()

class CallSession:
    """Manages a single call session with STT, LLM, and TTS pipeline"""
    
    def __init__(self, call_id: str, agent_config: dict, agent_id: str = None, knowledge_base: str = "", user_id: str = None):
        self.call_id = call_id
        self.agent_config = agent_config
        self.agent_id = agent_id or agent_config.get("id")  # Store agent_id for refreshing config
        self.user_id = user_id or agent_config.get("user_id")  # Store user_id for API key retrieval
        self.conversation_history = []
        self.current_node_id = None  # Track current position in flow
        self.should_end_call = False  # Flag for ending nodes
        self.session_variables = {}  # Store variables from webhooks and extractions
        self.knowledge_base = knowledge_base  # Store KB content for LLM context
        
        # Voicemail/IVR detection
        from voicemail_detector import VoicemailDetector
        self.voicemail_detector = VoicemailDetector(agent_config.get("settings", {}))
        
        # Add current date/time in EST timezone as {{now}} variable
        from datetime import datetime
        import pytz
        est = pytz.timezone('America/New_York')
        now_est = datetime.now(est)
        # Format: "Monday, November 4, 2025 at 3:45 PM EST"
        self.session_variables['now'] = now_est.strftime("%A, %B %-d, %Y at %-I:%M %p %Z")
        
        # Build cached system prompt ONCE with KB - critical for Grok's prefix caching
        # Grok caches based on exact prefix match, so we must reuse the SAME string object
        self._cached_system_prompt = self._build_cached_system_prompt()
        
        self.is_active = True
        self.deepgram_connection = None
        self.current_transcript = ""
        self.latency_start = None
        
        # Dead air prevention tracking
        self.silence_start_time = None  # When silence started (after agent stopped speaking)
        self.agent_speaking = False  # Track if agent is currently speaking
        self.user_speaking = False  # Track if user is currently speaking
        self.checkin_count = 0  # Current check-in count for this silence period
        self.hold_on_detected = False  # Whether user said "hold on" in last response
        self.call_start_time = time.time()  # Track call start for max duration
        self.silence_timer_task = None  # Background task for silence monitoring
        self.last_checkin_time = None  # Track last check-in to avoid rapid repeats
        self.max_checkins_reached = False  # Flag to indicate we've hit max and should end after next timeout
    
    def _build_cached_system_prompt(self):
        """Build the cached system prompt ONCE - KB added dynamically via smart routing"""
        
        # START WITH AGENT'S GLOBAL PROMPT (personality, behavior, rules)
        # This is the most important part - it defines the agent's character and boundaries
        global_prompt = self.agent_config.get("system_prompt", "").strip()
        
        # Then add technical communication rules
        technical_rules = """

# COMMUNICATION STYLE
- Speak naturally and conversationally
- Keep responses concise (1-2 sentences when possible)
- Use natural pauses, not XML tags
- Be friendly and professional

# STRICT RULES
- NO format markers or meta-text
- NO XML tags like <speak> or <break>
- NO phrases like '--- AI TURN ---' or 'AGENT_SCRIPT_LINE_INPUT:'
- Just speak naturally as a human would

# CRITICAL - AVOID REPETITION
- Review the conversation history carefully before responding
- DO NOT repeat questions or phrases you've already said
- If you've already asked about a specific dollar amount (like $20,000/month), DO NOT ask it again
- If you need to accomplish the same goal, come up with a DIFFERENT approach - rephrase, use different framing, or ask from a different angle
- Track what you've already said and ensure each response brings something NEW to the conversation"""
        
        # Combine: Global prompt (character/boundaries) + Technical rules (format/repetition)
        if global_prompt:
            prompt = global_prompt + technical_rules
            logger.info(f"üìã Built cached system prompt: {len(global_prompt)} chars (global) + {len(technical_rules)} chars (technical)")
        else:
            # Fallback if no global prompt defined
            prompt = "You are a phone agent conducting natural conversations." + technical_rules
            logger.info(f"‚ö†Ô∏è No global prompt found - using default")
        
        # NOTE: KB is added dynamically via smart routing, not in cached prompt
        # This keeps the cached prompt clean and enables sub-500ms simple chat responses
        if self.knowledge_base:
            logger.info(f"üìö KB available - will use smart routing (simple chat: NO KB, factual: WITH KB)")
        
        return prompt
    
    async def refresh_agent_config(self, db):
        """Refresh agent configuration from database to get latest settings
        
        Uses PRIMARY read preference to ensure no replica lag and always gets
        the most recent data, even immediately after UI updates.
        """
        try:
            if not self.agent_id:
                logger.warning(f"Cannot refresh agent config: agent_id not set for call {self.call_id}")
                return
            
            # Fetch latest agent config from database with PRIMARY read preference
            # This ensures we get the most recent data, avoiding replica lag
            from pymongo import ReadPreference
            agent = await db.agents.with_options(
                read_preference=ReadPreference.PRIMARY
            ).find_one({"id": self.agent_id})
            
            if agent:
                self.agent_config = agent
                settings = agent.get('settings', {})
                tts_provider = settings.get('tts_provider', 'NOT SET')
                logger.info(f"‚úÖ Refreshed agent config for call {self.call_id}, agent {self.agent_id}")
                logger.info(f"   üîä TTS Provider: {tts_provider}")
                
                # Log provider-specific settings
                if tts_provider == 'elevenlabs':
                    voice_id = settings.get('elevenlabs_settings', {}).get('voice_id', 'N/A')
                    logger.info(f"   üéôÔ∏è ElevenLabs Voice ID: {voice_id}")
                elif tts_provider == 'cartesia':
                    voice_id = settings.get('voice_id', 'N/A')
                    logger.info(f"   üéôÔ∏è Cartesia Voice ID: {voice_id}")
                elif tts_provider == 'melo':
                    melo_voice = settings.get('melo_settings', {}).get('voice', 'N/A')
                    melo_speed = settings.get('melo_settings', {}).get('speed', 'N/A')
                    logger.info(f"   üéôÔ∏è MeloTTS Voice: {melo_voice}, Speed: {melo_speed}")
            else:
                logger.warning(f"Agent {self.agent_id} not found in database, keeping cached config")
        except Exception as e:
            logger.error(f"Error refreshing agent config: {e}, keeping cached config")
        
    
    def _detect_hold_on_phrase(self, text: str) -> bool:
        """Detect if user said 'hold on' or similar phrase"""
        hold_on_phrases = [
            "hold on", "wait", "one moment", "give me a second", 
            "hang on", "just a sec", "one sec", "hold please"
        ]
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in hold_on_phrases)
    
    def start_silence_tracking(self):
        """Start tracking silence after agent stops speaking"""
        if not self.agent_speaking and not self.user_speaking:
            self.silence_start_time = time.time()
            logger.info(f"üîá Silence tracking started for call {self.call_id}")
    
    def reset_silence_tracking(self):
        """Reset silence tracking when user starts speaking"""
        self.silence_start_time = None
        self.checkin_count = 0  # Reset check-in count when user responds
        self.hold_on_detected = False
        self.max_checkins_reached = False  # Reset the flag when user responds
        logger.info(f"üîä Silence tracking reset for call {self.call_id} (user responded)")
    
    def mark_agent_speaking_start(self):
        """Mark that agent has started speaking"""
        self.agent_speaking = True
        self.silence_start_time = None  # Stop counting silence
        logger.debug(f"ü§ñ Agent started speaking for call {self.call_id}")
    
    def mark_agent_speaking_end(self):
        """Mark that agent has stopped speaking - start silence timer"""
        self.agent_speaking = False
        # Start silence tracking immediately after agent stops
        if not self.user_speaking:
            self.start_silence_tracking()
        logger.debug(f"ü§ñ Agent stopped speaking for call {self.call_id}")
    
    def mark_user_speaking_start(self):
        """Mark that user has started speaking"""
        self.user_speaking = True
        self.reset_silence_tracking()  # Reset when user starts
        logger.debug(f"üë§ User started speaking for call {self.call_id}")
    
    def mark_user_speaking_end(self):
        """Mark that user has stopped speaking"""
        self.user_speaking = False
        # Don't start silence tracking yet - wait for agent to finish responding
        logger.debug(f"üë§ User stopped speaking for call {self.call_id}")
    
    def get_silence_duration(self) -> float:
        """Get current silence duration in seconds"""
        if self.silence_start_time and not self.agent_speaking and not self.user_speaking:
            return time.time() - self.silence_start_time
        return 0.0
    
    def should_checkin(self) -> bool:
        """Check if we should send a check-in message"""
        settings = self.agent_config.get("settings", {}).get("dead_air_settings", {})
        silence_timeout = settings.get("silence_timeout_hold_on", 25) if self.hold_on_detected else settings.get("silence_timeout_normal", 7)
        max_checkins = settings.get("max_checkins_before_disconnect", 2)
        
        # If we've already reached max check-ins, don't send more check-ins
        # (but we'll still wait for the timeout to end the call)
        if self.checkin_count >= max_checkins:
            return False
        
        # Check silence duration
        silence_duration = self.get_silence_duration()
        if silence_duration >= silence_timeout:
            # Prevent rapid check-ins (at least 3 seconds between check-ins)
            if self.last_checkin_time is None or (time.time() - self.last_checkin_time) >= 3:
                logger.info(f"‚è∞ Check-in triggered after {silence_duration:.1f}s silence (threshold: {silence_timeout}s)")
                return True
        
        return False
    
    def should_end_call_max_duration(self) -> bool:
        """Check if call should end due to max duration"""
        settings = self.agent_config.get("settings", {}).get("dead_air_settings", {})
        max_duration = settings.get("max_call_duration", 1500)  # 25 minutes default
        
        call_duration = time.time() - self.call_start_time
        if call_duration >= max_duration:
            logger.warning(f"‚è±Ô∏è Max call duration ({max_duration}s) reached for call {self.call_id}")
            return True
        return False
    
    def should_end_call_max_checkins(self) -> bool:
        """Check if call should end due to max check-ins
        
        After max check-ins are sent, we wait one more silence period.
        If still no response after that period, end the call.
        """
        settings = self.agent_config.get("settings", {}).get("dead_air_settings", {})
        max_checkins = settings.get("max_checkins_before_disconnect", 2)
        silence_timeout = settings.get("silence_timeout_hold_on", 25) if self.hold_on_detected else settings.get("silence_timeout_normal", 7)
        
        # If we've reached max check-ins
        if self.checkin_count >= max_checkins and not self.max_checkins_reached:
            # Set flag - we'll wait one more period before ending
            self.max_checkins_reached = True
            logger.warning(f"‚ö†Ô∏è Max check-ins ({max_checkins}) reached - will end call after one more {silence_timeout}s silence period")
            return False
        
        # If flag is set AND we've waited the additional silence period
        if self.max_checkins_reached:
            silence_duration = self.get_silence_duration()
            if silence_duration >= silence_timeout:
                logger.warning(f"üö´ Call ending - max check-ins reached AND additional {silence_timeout}s timeout expired")
                return True
        
        return False
    
    def get_checkin_message(self) -> str:
        """Get the check-in message to send"""
        settings = self.agent_config.get("settings", {}).get("dead_air_settings", {})
        max_checkins = settings.get("max_checkins_before_disconnect", 2)
        message = settings.get("checkin_message", "Are you still there?")
        self.checkin_count += 1
        self.last_checkin_time = time.time()
        
        # If this is the last check-in, set the flag
        if self.checkin_count >= max_checkins:
            self.max_checkins_reached = True
            logger.info(f"üí¨ Sending FINAL check-in #{self.checkin_count}/{max_checkins}: {message}")
            logger.info(f"‚ö†Ô∏è Will end call if no response after next silence timeout")
        else:
            logger.info(f"üí¨ Sending check-in #{self.checkin_count}/{max_checkins}: {message}")
        
        return message

    async def initialize_deepgram(self):
        """Initialize Deepgram live transcription"""
        try:
            # For now, we'll use a simplified approach without real-time STT
            # In production, you would set up the Deepgram WebSocket connection here
            logger.info(f"Deepgram initialized for call {self.call_id}")
            self.deepgram_connection = None  # Placeholder
            
        except Exception as e:
            logger.error(f"Error initializing Deepgram: {e}")
            raise
    
    async def on_transcript(self, result):
        """Handle transcription results from Deepgram"""
        try:
            transcript = result.channel.alternatives[0].transcript
            
            if len(transcript) > 0:
                is_final = result.is_final
                
                if is_final:
                    logger.info(f"Final transcript: {transcript}")
                    self.current_transcript = transcript
                    
                    # Process the complete utterance
                    await self.process_user_input(transcript)
                    self.current_transcript = ""
                else:
                    # Update current transcript for interim results
                    self.current_transcript = transcript
                    
        except Exception as e:
            logger.error(f"Error processing transcript: {e}")
    
    def on_error(self, error):
        """Handle Deepgram errors"""
        logger.error(f"Deepgram error: {error}")
    
    async def process_user_input(self, user_text: str, stream_callback=None):
        """Process user input through LLM and generate response
        
        Args:
            user_text: User's transcribed text
            stream_callback: Optional callback for streaming sentences as they're generated
        """
        try:
            latency_start = time.time()
            
            # Update {{now}} variable with current date/time in EST
            from datetime import datetime
            import pytz
            est = pytz.timezone('America/New_York')
            now_est = datetime.now(est)
            self.session_variables['now'] = now_est.strftime("%A, %B %-d, %Y at %-I:%M %p %Z")
            
            # Note: conversation_history is already set in server.py if provided
            # Only add current message if it's not already in history
            if not self.conversation_history or self.conversation_history[-1]["content"] != user_text:
                self.conversation_history.append({
                    "role": "user",
                    "content": user_text
                })
            
            # Determine which logic to use based on agent type
            agent_type = self.agent_config.get("agent_type", "single_prompt")
            
            if agent_type == "call_flow":
                # Call Flow Agent - use flow nodes with streaming
                assistant_response = await self._process_call_flow_streaming(user_text, stream_callback)
            else:
                # Single Prompt Agent - use system prompt with streaming
                assistant_response = await self._process_single_prompt_streaming(user_text, stream_callback)
            
            # Add assistant response to conversation history
            assistant_msg = {
                "role": "assistant",
                "content": assistant_response
            }
            
            # For call flow agents, include the current node ID for state tracking
            if agent_type == "call_flow" and self.current_node_id:
                assistant_msg["_node_id"] = self.current_node_id
                logger.info(f"üíæ Storing node ID {self.current_node_id} in conversation history")
            
            self.conversation_history.append(assistant_msg)
            
            # Calculate LLM latency
            llm_latency = time.time() - latency_start
            logger.info(f"LLM response ({llm_latency:.2f}s): {assistant_response}")
            
            # Calculate total latency
            total_latency = time.time() - latency_start
            logger.info(f"Total latency: {total_latency:.2f}s")
            
            return {
                "text": assistant_response,
                "latency": total_latency,
                "end_call": self.should_end_call  # Signal if call should end
            }
            
        except Exception as e:
            logger.error(f"Error processing user input: {e}")
            return None
    
    async def _process_single_prompt_streaming(self, user_text: str, stream_callback=None):
        """Process with single prompt mode and stream sentences"""
        import re
        
        # Build conversation history
        messages = []
        system_prompt = self.agent_config.get("system_prompt", "You are a helpful assistant.")
        
        # Add knowledge base to system prompt if available
        if self.knowledge_base:
            system_prompt += f"\n\n=== KNOWLEDGE BASE ===\nYou have access to multiple reference sources below. Each source serves a different purpose.\n\nüß† HOW TO USE THE KNOWLEDGE BASE:\n1. When user asks a question, FIRST identify which knowledge base source(s) are relevant based on their descriptions\n2. Read ONLY the relevant source(s) to find the answer\n3. Use ONLY information from the knowledge base - do NOT make up or improvise ANY factual details\n4. If the knowledge base doesn't contain the answer, say: \"I don't have that specific information available\"\n5. Different sources contain different types of information - match the user's question to the right source\n\n‚ö†Ô∏è NEVER invent: company names, product names, prices, processes, methodologies, or any factual information not in the knowledge base\n\n{self.knowledge_base}\n=== END KNOWLEDGE BASE ===\n"
        
        messages.append({"role": "system", "content": system_prompt})
        messages.extend(self.conversation_history)
        
        llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
        model = self.agent_config.get("settings", {}).get("model", "gpt-4.1-2025-04-14")
        
        logger.info(f"ü§ñ Using LLM provider: {llm_provider}, model: {model}")
        
        # Get appropriate client
        if llm_provider == "grok":
            from server import get_api_key
            grok_key = await get_api_key("grok")
            client = await get_llm_client("grok", api_key=grok_key)
        else:
            client = get_openai_client()
        
        if not client:
            raise Exception(f"{llm_provider} client not configured")
        
        # Stream LLM response and process sentence by sentence
        llm_request_start = time.time()
        
        if llm_provider == "grok":
            response = await client.create_completion(
                messages=messages,
                model=model,
                temperature=self.agent_config.get("settings", {}).get("temperature", 0.7),
                max_tokens=self.agent_config.get("settings", {}).get("max_tokens", 300),
                stream=True
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=self.agent_config.get("settings", {}).get("temperature", 0.7),
                max_tokens=self.agent_config.get("settings", {}).get("max_tokens", 300),
                stream=True
            )
        
        # Collect response and stream sentences
        full_response = ""
        sentence_buffer = ""
        first_token_received = False
        
        # Sentence delimiters
        sentence_endings = re.compile(r'([.!?]\s+)')
        
        async for chunk in response:
            if not first_token_received:
                ttft_ms = int((time.time() - llm_request_start) * 1000)
                logger.info(f"‚è±Ô∏è  LLM TTFT: {ttft_ms}ms ({llm_provider} {model})")
                first_token_received = True
            
            # Extract content from chunk
            if llm_provider == "grok":
                if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                    else:
                        continue
                else:
                    continue
            else:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                else:
                    continue
            
            full_response += content
            sentence_buffer += content
            
            # Check if we have a complete sentence
            if sentence_endings.search(sentence_buffer):
                # Split into sentences
                sentences = sentence_endings.split(sentence_buffer)
                
                # Process complete sentences (leave last incomplete one in buffer)
                for i in range(0, len(sentences) - 1, 2):
                    if i < len(sentences):
                        sentence = sentences[i]
                        if i + 1 < len(sentences):
                            sentence += sentences[i + 1]  # Add delimiter
                        
                        sentence = sentence.strip()
                        if sentence and stream_callback:
                            # Stream this sentence immediately to TTS
                            await stream_callback(sentence)
                            logger.info(f"üì§ Streamed sentence: {sentence[:50]}...")
                
                # Keep the last incomplete part in buffer
                sentence_buffer = sentences[-1] if len(sentences) % 2 != 0 else ""
        
        # Send any remaining text
        if sentence_buffer.strip() and stream_callback:
            await stream_callback(sentence_buffer.strip())
            logger.info(f"üì§ Streamed final fragment: {sentence_buffer[:50]}...")
        
        return full_response
    
    async def _process_single_prompt(self, user_message: str) -> str:
        """Process using traditional single system prompt"""
        try:
            system_prompt = self.agent_config.get("system_prompt", "You are a helpful AI assistant.")
            
            # Add knowledge base to system prompt if available
            if self.knowledge_base:
                system_prompt += f"\n\n=== KNOWLEDGE BASE ===\nYou have access to multiple reference sources below. Each source serves a different purpose.\n\nüß† HOW TO USE THE KNOWLEDGE BASE:\n1. When user asks a question, FIRST identify which knowledge base source(s) are relevant based on their descriptions\n2. Read ONLY the relevant source(s) to find the answer\n3. Use ONLY information from the knowledge base - do NOT make up or improvise ANY factual details\n4. If the knowledge base doesn't contain the answer, say: \"I don't have that specific information available\"\n5. Different sources contain different types of information - match the user's question to the right source\n\n‚ö†Ô∏è NEVER invent: company names, product names, prices, processes, methodologies, or any factual information not in the knowledge base\n\n{self.knowledge_base}\n=== END KNOWLEDGE BASE ===\n"
            
            messages = [
                {"role": "system", "content": system_prompt}
            ] + self.conversation_history  # Full history with 2M context window
            
            # Get LLM provider from agent settings
            llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
            model = self.agent_config.get("model", "gpt-4-turbo")
            
            # Validate model matches provider and fix if mismatched
            grok_models = ["grok-4-fast-non-reasoning", "grok-4-fast-reasoning", "grok-3", "grok-2-1212", "grok-beta", "grok-4-fast"]
            openai_models = ["gpt-4.1-2025-04-14", "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo"]
            
            if llm_provider == "grok":
                if model not in grok_models:
                    logger.warning(f"‚ö†Ô∏è  Model '{model}' not valid for Grok, using 'grok-3'")
                    model = "grok-3"
            else:
                if model in grok_models:
                    logger.warning(f"‚ö†Ô∏è  Model '{model}' is a Grok model but provider is OpenAI, using 'gpt-4-turbo'")
                    model = "gpt-4-turbo"
            
            logger.info(f"ü§ñ Using LLM provider: {llm_provider}, model: {model}")
            
            # Get appropriate client
            if llm_provider == "grok":
                from server import get_api_key
                grok_key = await get_api_key("grok")
                client = await get_llm_client("grok", api_key=grok_key)
            else:
                client = get_openai_client()
                
            if not client:
                raise Exception(f"{llm_provider} client not configured")
            
            # ‚è±Ô∏è  Track LLM TTFT (Time-To-First-Token)
            llm_request_start = time.time()
            
            # Use the appropriate client method WITH STREAMING for faster TTFT
            if llm_provider == "grok":
                response = await client.create_completion(
                    messages=messages,
                    model=model,
                    temperature=self.agent_config.get("settings", {}).get("temperature", 0.7),
                    max_tokens=self.agent_config.get("settings", {}).get("max_tokens", 500),
                    stream=True
                )
                
                # Collect streamed response
                full_response = ""
                first_token_received = False
                async for chunk in response:
                    if not first_token_received:
                        ttft_ms = int((time.time() - llm_request_start) * 1000)
                        logger.info(f"‚è±Ô∏è  LLM TTFT: {ttft_ms}ms ({llm_provider} {model})")
                        first_token_received = True
                    
                    if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content:
                            full_response += delta.content
                
                return full_response
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=self.agent_config.get("settings", {}).get("temperature", 0.7),
                    max_tokens=self.agent_config.get("settings", {}).get("max_tokens", 500),
                    stream=True
                )
                
                # Collect streamed response and track TTFT
                full_response = ""
                first_token_received = False
                async for chunk in response:
                    if not first_token_received:
                        ttft_ms = int((time.time() - llm_request_start) * 1000)
                        logger.info(f"‚è±Ô∏è  LLM TTFT: {ttft_ms}ms ({llm_provider} {model})")
                        first_token_received = True
                    
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                
                return full_response
        except Exception as e:
            logger.error(f"Error in single prompt: {e}")
            return "I apologize, but I'm having trouble processing your request."
    
    async def _process_call_flow_streaming(self, user_message: str, stream_callback=None) -> str:
        """Process using call flow nodes with streaming support"""
        try:
            flow_nodes = self.agent_config.get("call_flow", [])
            
            if not flow_nodes:
                logger.warning("No flow nodes, falling back to single prompt streaming")
                return await self._process_single_prompt_streaming(user_message, stream_callback)
            
            selected_node = None
            
            # Determine if this is first message based on history length BEFORE adding current message
            # (conversation_history gets set in server.py, then current message added in process_user_input)
            is_first_message = len(self.conversation_history) <= 1
            
            logger.info(f"Flow processing - History length: {len(self.conversation_history)}, Is first: {is_first_message}")
            
            # First message - check start node settings
            if is_first_message:
                # Find start node
                start_node = None
                for node in flow_nodes:
                    if node.get("type", "").lower() == "start":
                        start_node = node
                        break
                
                # Handle start node settings
                if start_node:
                    start_data = start_node.get("data", {})
                    who_speaks_first = start_data.get("whoSpeaksFirst", "user")
                    
                    # If AI speaks first, find first conversation node
                    if who_speaks_first == "ai":
                        for node in flow_nodes:
                            if node.get("type") == "conversation":
                                selected_node = node
                                break
                    # If user speaks first, we're in this function, so find matching node
                
                # Find first interactive node for response (conversation, collect_input, press_digit, or extract_variable)
                if not selected_node:
                    for node in flow_nodes:
                        node_type = node.get("type", "").lower()
                        if node_type in ["conversation", "collect_input", "press_digit", "extract_variable"]:
                            selected_node = node
                            break
            else:
                # Subsequent messages - follow transitions from current node
                # Extract last node from conversation history
                current_node_id = None
                if self.conversation_history:
                    for msg in reversed(self.conversation_history):
                        if msg.get("role") == "assistant" and "_node_id" in msg:
                            current_node_id = msg.get("_node_id")
                            break
                
                logger.info(f"üîç Current node ID from history: {current_node_id}")
                
                # Find the current node
                current_node = None
                if current_node_id:
                    for node in flow_nodes:
                        if node.get("id") == current_node_id:
                            current_node = node
                            break
                
                # If no current node found, start from first conversation node
                if not current_node:
                    logger.warning("No current node in history, starting from first conversation node")
                    current_node = await self._get_first_conversation_node(flow_nodes)
                
                if current_node:
                    # Check if current node is a collect_input node that needs processing
                    current_node_type = current_node.get("type", "")
                    if current_node_type == "collect_input":
                        # Process collect_input node first
                        selected_node = current_node
                    else:
                        # For other node types, follow transitions
                        logger.info(f"Evaluating transitions from {current_node.get('label')} for message: {user_message}")
                        selected_node = await self._follow_transition(current_node, user_message, flow_nodes)
                else:
                    selected_node = await self._get_first_conversation_node(flow_nodes)
            
            if selected_node:
                # Update current node position
                self.current_node_id = selected_node.get("id")
                node_data = selected_node.get("data", {})
                node_type = selected_node.get("type", "")
                
                # Get the actual script content, not the prompt field
                if node_type == "conversation":
                    content = node_data.get("script", "") or node_data.get("content", "")
                else:
                    content = node_data.get("content", "") or node_data.get("script", "")
                
                # Replace variables in content BEFORE processing
                for var_name, var_value in self.session_variables.items():
                    content = content.replace(f"{{{{{var_name}}}}}", str(var_value))
                    logger.info(f"üîß Replaced {{{{{var_name}}}}} with {var_value} in content")
                
                # Smart detection of mode: if content is very long or has instructions, it's "prompt" mode
                # Get mode from data (can be "prompt" or "script") - check both possible field names
                prompt_type = node_data.get("mode")
                if prompt_type is None:
                    prompt_type = node_data.get("promptType")
                
                # ALWAYS auto-detect if mode is None, empty, or explicitly "script" but content suggests otherwise
                # This ensures large instruction nodes are properly detected even if misconfigured
                if prompt_type is None or prompt_type == "" or (
                    prompt_type == "script" and len(content) > 500
                ):
                    # If content is very long (>500 chars) or contains instruction markers, treat as prompt
                    if len(content) > 500 or any(marker in content.lower() for marker in [
                        "## ", "### ", "instructions:", "goal:", "objective:", "**important**", 
                        "you are", "your task", "rules:", "primary goal", "**no dashes"
                    ]):
                        prompt_type = "prompt"
                        logger.info(f"üîç Auto-detected PROMPT mode (length: {len(content)} chars, overriding node setting)")
                    else:
                        prompt_type = "script"
                        logger.info(f"üîç Auto-detected SCRIPT mode (length: {len(content)} chars)")
                
                logger.info(f"Using flow node: {selected_node.get('label', 'unnamed')} (type: {node_type}, mode: {prompt_type})")
                
                # Handle function/webhook node FIRST - before any content processing
                # Function nodes don't need content, they execute webhooks
                if node_type == "function":
                    logger.info("üîß Function/Webhook node reached - executing webhook")
                    
                    # Check if we should speak during execution
                    speak_during_execution = node_data.get("speak_during_execution", False)
                    dialogue_text = node_data.get("dialogue_text", "")
                    dialogue_type = node_data.get("dialogue_type", "static")
                    wait_for_result = node_data.get("wait_for_result", True)
                    
                    dialogue_response = ""
                    
                    # Generate and speak dialogue if enabled
                    if speak_during_execution and dialogue_text:
                        logger.info(f"üí¨ Generating dialogue before webhook execution (type: {dialogue_type})")
                        
                        if dialogue_type == "static":
                            # Use exact text provided
                            dialogue_response = dialogue_text
                            logger.info(f"üì¢ Static dialogue: {dialogue_response[:50]}...")
                        else:
                            # AI-generated dialogue based on prompt
                            logger.info(f"ü§ñ Generating AI dialogue from prompt: {dialogue_text[:50]}...")
                            dialogue_response = await self._generate_ai_response_streaming(dialogue_text, stream_callback)
                        
                        # Stream the dialogue if callback provided
                        if stream_callback and dialogue_response:
                            await stream_callback(dialogue_response)
                    
                    # Execute webhook based on wait_for_result setting
                    if wait_for_result:
                        # Wait for webhook to complete before transitioning
                        logger.info("‚è≥ Waiting for webhook to complete before transitioning...")
                        webhook_response = await self._execute_webhook(node_data, user_message)
                        
                        # Check if webhook requires re-prompt (missing required variables)
                        if webhook_response.get("requires_reprompt"):
                            logger.warning("üîÅ Webhook requires re-prompt - staying on same node")
                            # Don't transition, return the re-prompt message
                            return webhook_response.get("message", "I need more information to proceed.")
                    else:
                        # Execute webhook async and transition immediately
                        logger.info("üöÄ Executing webhook async, transitioning immediately...")
                        import asyncio
                        asyncio.create_task(self._execute_webhook(node_data, user_message))
                        webhook_response = {"success": True, "message": "Webhook executing in background"}
                    
                    # After webhook (or immediately if not waiting), transition to next node
                    if node_data.get("transitions"):
                        next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                        if next_node and next_node.get("id") != selected_node.get("id"):
                            # Recursively process the next node
                            self.current_node_id = next_node.get("id")
                            return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
                    
                    # If we had dialogue, return that, otherwise return webhook message
                    if dialogue_response:
                        return dialogue_response
                    else:
                        return webhook_response.get("message", "Function executed successfully")
                
                # Handle ending node - special case
                if node_type == "ending":
                    logger.info("üõë Ending node reached - call should terminate")
                    # Add a marker that call should end
                    self.should_end_call = True
                
                # Handle transfer call node - special case
                if node_type in ["call_transfer", "agent_transfer"]:
                    logger.info("üìû Transfer node reached - initiating transfer")
                    transfer_info = self._handle_transfer(node_data)
                    
                    # Store transfer request in session variables
                    self.session_variables["transfer_requested"] = True
                    self.session_variables["transfer_info"] = transfer_info
                    
                    # Return transfer message
                    return transfer_info.get("message", "Please hold while I transfer your call...")
                
                # Handle collect input node - special case
                if node_type == "collect_input":
                    logger.info("üìù Collect Input node reached - gathering user input")
                    collect_result = self._handle_collect_input(node_data, user_message)
                    
                    if collect_result.get("valid"):
                        # Store collected value and move to next node
                        variable_name = node_data.get("variable_name", "user_input")
                        self.session_variables[variable_name] = collect_result.get("value")
                        logger.info(f"‚úÖ Collected and stored {variable_name}: {collect_result.get('value')}")
                        
                        # Transition to next node if available
                        if node_data.get("transitions"):
                            next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                            if next_node and next_node.get("id") != selected_node.get("id"):
                                self.current_node_id = next_node.get("id")
                                return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
                        
                        return collect_result.get("success_message", "Thank you, I have that information.")
                    else:
                        # Invalid input - prompt again
                        logger.info(f"‚ùå Invalid input: {collect_result.get('error')}")
                        return collect_result.get("error_message", "I didn't understand that. Please try again.")
                
                # Handle send SMS node - special case
                if node_type == "send_sms":
                    logger.info("üì± Send SMS node reached - sending message")
                    sms_result = await self._handle_send_sms(node_data, user_message)
                    
                    # Store SMS status in session variables
                    self.session_variables["sms_sent"] = sms_result.get("success", False)
                    self.session_variables["sms_status"] = sms_result.get("status", "pending")
                    
                    # Transition to next node if available
                    if node_data.get("transitions"):
                        next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                        if next_node and next_node.get("id") != selected_node.get("id"):
                            self.current_node_id = next_node.get("id")
                            return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
                    
                    return sms_result.get("message", "I've sent you an SMS.")
                
                # Handle logic split node - special case
                if node_type == "logic_split":
                    logger.info("üîÄ Logic Split node reached - evaluating conditions")
                    
                    # Evaluate conditions and get next node
                    next_node_id = self._evaluate_logic_conditions(node_data)
                    
                    if next_node_id:
                        next_node = self._get_node_by_id(next_node_id, flow_nodes)
                        if next_node:
                            logger.info(f"‚úÖ Condition matched - moving to node: {next_node.get('label')}")
                            self.current_node_id = next_node_id
                            return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
                    
                    # No condition matched - return default message
                    return "Let me help you with that."
                
                # Handle press digit node - special case
                if node_type == "press_digit":
                    logger.info("üî¢ Press Digit node reached - processing DTMF input")
                    digit_result = self._handle_press_digit(node_data, user_message)
                    
                    if digit_result.get("next_node_id"):
                        next_node = self._get_node_by_id(digit_result["next_node_id"], flow_nodes)
                        if next_node:
                            logger.info(f"‚úÖ Digit {digit_result['digit']} pressed - routing to node: {next_node.get('label')}")
                            self.current_node_id = digit_result["next_node_id"]
                            return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
                    
                    return digit_result.get("message", "Please press a digit.")
                
                # Handle extract variable node - special case
                if node_type == "extract_variable":
                    logger.info("üìã Extract Variable node reached - extracting data")
                    extract_result = await self._handle_extract_variable(node_data, user_message)
                    
                    if extract_result.get("success"):
                        # Store extracted value
                        variable_name = node_data.get("variable_name", "extracted_data")
                        self.session_variables[variable_name] = extract_result.get("value")
                        logger.info(f"‚úÖ Extracted {variable_name}: {extract_result.get('value')}")
                        
                        # Transition to next node
                        if node_data.get("transitions"):
                            next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                            if next_node and next_node.get("id") != selected_node.get("id"):
                                self.current_node_id = next_node.get("id")
                                return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
                        
                        return extract_result.get("message", "Got it.")
                    
                    return "Could you please provide that information again?"
                
                # Handle script vs prompt mode
                if prompt_type == "script":
                    # Return exact script - stream it if callback provided
                    if stream_callback:
                        await stream_callback(content)
                        logger.info(f"üì§ Streamed script content: {content[:50]}...")
                    return content
                else:
                    # Prompt mode - use AI to generate response based on instructions with streaming
                    return await self._generate_ai_response_streaming(content, stream_callback)
            else:
                logger.warning("No suitable node found, falling back")
                return await self._process_single_prompt_streaming(user_message, stream_callback)
                
        except Exception as e:
            logger.error(f"Error in call flow streaming: {e}")
            return await self._process_single_prompt_streaming(user_message, stream_callback)
    
    async def _process_call_flow(self, user_message: str) -> str:
        """Process using call flow nodes"""
        try:
            flow_nodes = self.agent_config.get("call_flow", [])
            
            if not flow_nodes:
                logger.warning("No flow nodes, falling back to single prompt")
                return await self._process_single_prompt(user_message)
            
            selected_node = None
            
            # Determine if this is first message based on history length BEFORE adding current message
            # (conversation_history gets set in server.py, then current message added in process_user_input)
            is_first_message = len(self.conversation_history) <= 1
            
            logger.info(f"Flow processing - History length: {len(self.conversation_history)}, Is first: {is_first_message}")
            
            # First message - check start node settings
            if is_first_message:
                # Find start node
                start_node = None
                for node in flow_nodes:
                    if node.get("type", "").lower() == "start":
                        start_node = node
                        break
                
                # Handle start node settings
                if start_node:
                    start_data = start_node.get("data", {})
                    who_speaks_first = start_data.get("whoSpeaksFirst", "user")
                    
                    # If AI speaks first, find first conversation node
                    if who_speaks_first == "ai":
                        for node in flow_nodes:
                            if node.get("type") == "conversation":
                                selected_node = node
                                break
                    # If user speaks first, we're in this function, so find matching node
                
                # Find first interactive node for response (conversation, collect_input, press_digit, or extract_variable)
                if not selected_node:
                    for node in flow_nodes:
                        node_type = node.get("type", "").lower()
                        if node_type in ["conversation", "collect_input", "press_digit", "extract_variable"]:
                            selected_node = node
                            break
            else:
                # Subsequent messages - follow transitions from current node
                # Extract last node from conversation history
                current_node_id = None
                if self.conversation_history:
                    for msg in reversed(self.conversation_history):
                        if msg.get("role") == "assistant" and "_node_id" in msg:
                            current_node_id = msg.get("_node_id")
                            break
                
                logger.info(f"üîç Current node ID from history: {current_node_id}")
                
                # Find the current node
                current_node = None
                if current_node_id:
                    for node in flow_nodes:
                        if node.get("id") == current_node_id:
                            current_node = node
                            break
                
                # If no current node found, start from first conversation node
                if not current_node:
                    logger.warning("No current node in history, starting from first conversation node")
                    current_node = await self._get_first_conversation_node(flow_nodes)
                
                if current_node:
                    # Check if current node is a collect_input node that needs processing
                    current_node_type = current_node.get("type", "")
                    if current_node_type == "collect_input":
                        # Process collect_input node first
                        selected_node = current_node
                    else:
                        # For other node types, follow transitions
                        logger.info(f"Evaluating transitions from {current_node.get('label')} for message: {user_message}")
                        selected_node = await self._follow_transition(current_node, user_message, flow_nodes)
                else:
                    selected_node = await self._get_first_conversation_node(flow_nodes)
            
            if selected_node:
                # Update current node position
                self.current_node_id = selected_node.get("id")
                node_data = selected_node.get("data", {})
                node_type = selected_node.get("type", "")
                
                # Get the actual script content, not the prompt field
                if node_type == "conversation":
                    content = node_data.get("script", "") or node_data.get("content", "")
                else:
                    content = node_data.get("content", "") or node_data.get("script", "")
                
                # Replace variables in content BEFORE processing
                for var_name, var_value in self.session_variables.items():
                    content = content.replace(f"{{{{{var_name}}}}}", str(var_value))
                    logger.info(f"üîß Replaced {{{{{var_name}}}}} with {var_value} in content")
                
                # Smart detection of mode: if content is very long or has instructions, it's "prompt" mode
                # Get mode from data (can be "prompt" or "script") - check both possible field names
                prompt_type = node_data.get("mode")
                if prompt_type is None:
                    prompt_type = node_data.get("promptType")
                
                # ALWAYS auto-detect if mode is None, empty, or explicitly "script" but content suggests otherwise
                # This ensures large instruction nodes are properly detected even if misconfigured
                if prompt_type is None or prompt_type == "" or (
                    prompt_type == "script" and len(content) > 500
                ):
                    # If content is very long (>500 chars) or contains instruction markers, treat as prompt
                    if len(content) > 500 or any(marker in content.lower() for marker in [
                        "## ", "### ", "instructions:", "goal:", "objective:", "**important**", 
                        "you are", "your task", "rules:", "primary goal", "**no dashes"
                    ]):
                        prompt_type = "prompt"
                        logger.info(f"üîç Auto-detected PROMPT mode (length: {len(content)} chars, overriding node setting)")
                    else:
                        prompt_type = "script"
                        logger.info(f"üîç Auto-detected SCRIPT mode (length: {len(content)} chars)")
                
                logger.info(f"Using flow node: {selected_node.get('label', 'unnamed')} (type: {node_type}, mode: {prompt_type})")
                
                # Handle ending node - special case
                if node_type == "ending":
                    logger.info("üõë Ending node reached - call should terminate")
                    # Add a marker that call should end
                    self.should_end_call = True
                
                # Handle transfer call node - special case
                if node_type in ["call_transfer", "agent_transfer"]:
                    logger.info("üìû Transfer node reached - initiating transfer")
                    transfer_info = self._handle_transfer(node_data)
                    
                    # Store transfer request in session variables
                    self.session_variables["transfer_requested"] = True
                    self.session_variables["transfer_info"] = transfer_info
                    
                    # Return transfer message
                    return transfer_info.get("message", "Please hold while I transfer your call...")
                
                # Handle collect input node - special case
                if node_type == "collect_input":
                    logger.info("üìù Collect Input node reached - gathering user input")
                    collect_result = self._handle_collect_input(node_data, user_message)
                    
                    if collect_result.get("valid"):
                        # Store collected value and move to next node
                        variable_name = node_data.get("variable_name", "user_input")
                        self.session_variables[variable_name] = collect_result.get("value")
                        logger.info(f"‚úÖ Collected and stored {variable_name}: {collect_result.get('value')}")
                        
                        # Transition to next node if available
                        if node_data.get("transitions"):
                            next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                            if next_node and next_node.get("id") != selected_node.get("id"):
                                self.current_node_id = next_node.get("id")
                                return await self._process_node_content(next_node, user_message, flow_nodes)
                        
                        return collect_result.get("success_message", "Thank you, I have that information.")
                    else:
                        # Invalid input - prompt again
                        logger.info(f"‚ùå Invalid input: {collect_result.get('error')}")
                        return collect_result.get("error_message", "I didn't understand that. Please try again.")
                
                # Handle send SMS node - special case
                if node_type == "send_sms":
                    logger.info("üì± Send SMS node reached - sending message")
                    sms_result = await self._handle_send_sms(node_data, user_message)
                    
                    # Store SMS status in session variables
                    self.session_variables["sms_sent"] = sms_result.get("success", False)
                    self.session_variables["sms_status"] = sms_result.get("status", "pending")
                    
                    # Transition to next node if available
                    if node_data.get("transitions"):
                        next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                        if next_node and next_node.get("id") != selected_node.get("id"):
                            self.current_node_id = next_node.get("id")
                            return await self._process_node_content(next_node, user_message, flow_nodes)
                    
                    return sms_result.get("message", "I've sent you an SMS.")
                
                # Handle logic split node - special case
                if node_type == "logic_split":
                    logger.info("üîÄ Logic Split node reached - evaluating conditions")
                    
                    # Evaluate conditions and get next node
                    next_node_id = self._evaluate_logic_conditions(node_data)
                    
                    if next_node_id:
                        next_node = self._get_node_by_id(next_node_id, flow_nodes)
                        if next_node:
                            logger.info(f"‚úÖ Condition matched - moving to node: {next_node.get('label')}")
                            self.current_node_id = next_node_id
                            return await self._process_node_content(next_node, user_message, flow_nodes)
                    
                    # No condition matched - return default message
                    return "Let me help you with that."
                
                # Handle press digit node - special case
                if node_type == "press_digit":
                    logger.info("üî¢ Press Digit node reached - processing DTMF input")
                    digit_result = self._handle_press_digit(node_data, user_message)
                    
                    if digit_result.get("next_node_id"):
                        next_node = self._get_node_by_id(digit_result["next_node_id"], flow_nodes)
                        if next_node:
                            logger.info(f"‚úÖ Digit {digit_result['digit']} pressed - routing to node: {next_node.get('label')}")
                            self.current_node_id = digit_result["next_node_id"]
                            return await self._process_node_content(next_node, user_message, flow_nodes)
                    
                    return digit_result.get("message", "Please press a digit.")
                
                # Handle extract variable node - special case
                if node_type == "extract_variable":
                    logger.info("üìã Extract Variable node reached - extracting data")
                    extract_result = await self._handle_extract_variable(node_data, user_message)
                    
                    if extract_result.get("success"):
                        # Store extracted value
                        variable_name = node_data.get("variable_name", "extracted_data")
                        self.session_variables[variable_name] = extract_result.get("value")
                        logger.info(f"‚úÖ Extracted {variable_name}: {extract_result.get('value')}")
                        
                        # Transition to next node
                        if node_data.get("transitions"):
                            next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                            if next_node and next_node.get("id") != selected_node.get("id"):
                                self.current_node_id = next_node.get("id")
                                return await self._process_node_content(next_node, user_message, flow_nodes)
                        
                        return extract_result.get("message", "Got it.")
                    
                    return "Could you please provide that information again?"
                
                # Handle function/webhook node - special case
                if node_type == "function":
                    logger.info("üîß Function/Webhook node reached - executing webhook")
                    
                    # Check if we should speak during execution
                    speak_during_execution = node_data.get("speak_during_execution", False)
                    dialogue_text = node_data.get("dialogue_text", "")
                    dialogue_type = node_data.get("dialogue_type", "static")
                    wait_for_result = node_data.get("wait_for_result", True)
                    
                    dialogue_response = ""
                    
                    # Generate and speak dialogue if enabled
                    if speak_during_execution and dialogue_text:
                        logger.info(f"üí¨ Generating dialogue before webhook execution (type: {dialogue_type})")
                        
                        if dialogue_type == "static":
                            # Use exact text provided
                            dialogue_response = dialogue_text
                            logger.info(f"üì¢ Static dialogue: {dialogue_response[:50]}...")
                        else:
                            # AI-generated dialogue based on prompt
                            logger.info(f"ü§ñ Generating AI dialogue from prompt: {dialogue_text[:50]}...")
                            dialogue_response = await self._generate_ai_response(dialogue_text)
                    
                    # Execute webhook based on wait_for_result setting
                    if wait_for_result:
                        # Wait for webhook to complete before transitioning
                        logger.info("‚è≥ Waiting for webhook to complete before transitioning...")
                        webhook_response = await self._execute_webhook(node_data, user_message)
                        
                        # Check if webhook requires re-prompt (missing required variables)
                        if webhook_response.get("requires_reprompt"):
                            logger.warning("üîÅ Webhook requires re-prompt - staying on same node")
                            # Don't transition, return the re-prompt message
                            return webhook_response.get("message", "I need more information to proceed.")
                    else:
                        # Execute webhook async and transition immediately
                        logger.info("üöÄ Executing webhook async, transitioning immediately...")
                        import asyncio
                        asyncio.create_task(self._execute_webhook(node_data, user_message))
                        webhook_response = {"success": True, "message": "Webhook executing in background"}
                    
                    # After webhook (or immediately if not waiting), transition to next node
                    if node_data.get("transitions"):
                        next_node = await self._follow_transition(selected_node, user_message, flow_nodes)
                        if next_node and next_node.get("id") != selected_node.get("id"):
                            # Recursively process the next node
                            self.current_node_id = next_node.get("id")
                            return await self._process_node_content(next_node, user_message, flow_nodes)
                    
                    # If we had dialogue, return that, otherwise return webhook message
                    if dialogue_response:
                        return dialogue_response
                    else:
                        return webhook_response.get("message", "Function executed successfully")
                    
                    # If no transitions or same node, return a message
                    return webhook_response.get("message", "Function executed successfully")
                
                # Extract variables in real-time for conversation nodes
                if node_type == "conversation":
                    extract_variables = node_data.get("extract_variables", [])
                    if extract_variables and len(extract_variables) > 0:
                        logger.info(f"üîç Real-time extraction: {len(extract_variables)} variables configured in conversation node")
                        await self._extract_variables_realtime(extract_variables, user_message)
                
                # Handle script vs prompt mode
                if prompt_type == "prompt":
                    # PROMPT MODE: Always generate AI response based on instructions
                    # This ensures prompt-mode nodes respond to ANY user input, including objections
                    logger.info("ü§ñ PROMPT mode detected - will generate AI response for user input")
                    node_goal = node_data.get("goal", "")
                    instruction = node_goal if node_goal else content
                    
                    if not instruction:
                        instruction = "Continue the conversation naturally based on the node's purpose"
                    
                    # Generate AI response with streaming
                    full_response = ""
                    async for sentence in self._generate_ai_response_streaming(instruction, stream_callback):
                        full_response += sentence + " "
                    
                    return full_response.strip()
                    
                elif prompt_type == "script":
                    # Check if we're repeating the same script (stayed on same node)
                    # This happens when user's response didn't match any transition
                    is_repeating = False
                    last_user_message = ""
                    if len(self.conversation_history) >= 2:
                        # Check if the last assistant message was the same script
                        # AND get the last user message for context
                        for msg in reversed(self.conversation_history):
                            if msg.get("role") == "user" and not last_user_message:
                                last_user_message = msg.get("content", "")
                            if msg.get("role") == "assistant":
                                last_assistant_msg = msg.get("content", "")
                                # If the content is very similar or same, we're repeating
                                if last_assistant_msg.strip() == content.strip():
                                    is_repeating = True
                                    logger.info("üîÑ Detected script repetition")
                                break
                    
                    if is_repeating:
                        # User's response didn't match any transition
                        # Use global prompt for intelligent recovery if available
                        global_prompt = self.agent_config.get("system_prompt", "").strip()
                        
                        # Add knowledge base to global prompt if available
                        if self.knowledge_base:
                            global_prompt += f"\n\n=== KNOWLEDGE BASE ===\nYou have access to multiple reference sources below. Each source serves a different purpose.\n\nüß† HOW TO USE THE KNOWLEDGE BASE:\n1. When user asks a question, FIRST identify which knowledge base source(s) are relevant based on their descriptions\n2. Read ONLY the relevant source(s) to find the answer\n3. Use ONLY information from the knowledge base - do NOT make up or improvise ANY factual details\n4. If the knowledge base doesn't contain the answer, say: \"I don't have that specific information available\"\n5. Different sources contain different types of information - match the user's question to the right source\n\n‚ö†Ô∏è NEVER invent: company names, product names, prices, processes, methodologies, or any factual information not in the knowledge base\n\n{self.knowledge_base}\n=== END KNOWLEDGE BASE ===\n"
                        
                        node_goal = node_data.get("goal", "").strip()
                        
                        if global_prompt and node_goal:
                            logger.info("üåç Using global prompt + node goal for intelligent recovery")
                            
                            # Detect if user has a question or objection for better context
                            user_has_question = False
                            if last_user_message:
                                question_indicators = ["?", "how", "what", "why", "scam", "legit", "trust", "real", "catch", "cost", "price", "much"]
                                if any(indicator in last_user_message.lower() for indicator in question_indicators):
                                    user_has_question = True
                                    logger.info(f"ü§î User appears to have a question/objection: {last_user_message[:100]}")
                            
                            # Use AI to generate intelligent recovery based on global prompt + goal
                            try:
                                llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
                                model = self.agent_config.get("model", "gpt-4-turbo")
                                
                                if llm_provider == "grok":
                                    from server import get_api_key
                                    grok_key = await get_api_key("grok")
                                    client = await get_llm_client("grok", api_key=grok_key)
                                else:
                                    client = await get_llm_client("openai")
                                
                                if client:
                                    # Tailor the instruction based on whether it's a question or not
                                    if user_has_question:
                                        recovery_instruction = f"""The user responded with a question or concern instead of giving a direct answer.

Node Goal: {node_goal}
Original Script: {content}
User's Response: {last_user_message}

CRITICAL INSTRUCTION - AVOID REPETITION:
- Review the conversation history below carefully
- DO NOT repeat any questions or phrases you've already said in this conversation
- If you've already asked about a specific dollar amount (like $20,000/month), DO NOT ask it again
- If you need to circle back to the goal, come up with a DIFFERENT approach - rephrase, use different framing, or ask a different angle

Your task: 
1. Address their question or concern FIRST (briefly and naturally)
2. Then guide them back toward the goal using a FRESH approach (not what you already said)
3. Stay in character based on your persona

DO NOT just repeat the question. Handle their concern, THEN continue the conversation naturally with NEW phrasing."""
                                    else:
                                        recovery_instruction = f"""The user's response didn't quite match what was expected for this step in the conversation.

Node Goal: {node_goal}
Original Script: {content}
User's Response: {last_user_message}

CRITICAL INSTRUCTION - AVOID REPETITION:
- Review the conversation history below carefully
- DO NOT repeat any questions or phrases you've already said in this conversation
- If you've already asked about a specific dollar amount (like $20,000/month), DO NOT ask it again
- If you need to circle back to the goal, come up with a DIFFERENT approach - rephrase, use different framing, or ask a different angle

Your task: 
1. Acknowledge their response naturally
2. Guide them back toward the goal using a FRESH approach (without repeating the exact script OR what you already said)
3. Stay in character based on your persona

Use your natural conversational style to handle this smoothly with NEW phrasing."""

                                    var_context = ""
                                    if self.session_variables:
                                        var_context = f"\n\nAvailable context: {json.dumps(self.session_variables)}"
                                    
                                    # IMPORTANT: Include MORE conversation history (last 8 messages instead of 3)
                                    # This ensures the LLM can see what it already said and avoid repetition
                                    messages = [
                                        {"role": "system", "content": global_prompt + var_context},
                                        {"role": "system", "content": recovery_instruction}
                                    ] + self.conversation_history  # Full history
                                    
                                    if llm_provider == "grok":
                                        response = await client.create_completion(
                                            messages=messages,
                                            model=model,
                                            temperature=0.7,
                                            max_tokens=300  # Increased to allow more thoughtful responses
                                        )
                                    else:
                                        response = await client.chat.completions.create(
                                            model=model,
                                            messages=messages,
                                            temperature=0.7,
                                            max_tokens=300  # Increased to allow more thoughtful responses
                                        )
                                    
                                    recovery_response = response.choices[0].message.content.strip()
                                    if recovery_response:
                                        logger.info(f"‚úÖ Generated intelligent recovery: {recovery_response[:100]}...")
                                        return recovery_response
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Failed to generate intelligent recovery: {e}")
                        
                        # Fallback: Add clarifying phrase before repeating the question
                        # This only happens if no global prompt or no node goal
                        logger.info("‚ûï No global prompt available - adding simple clarification before repeating script")
                        clarifications = [
                            "Sorry, I didn't quite catch that. ",
                            "Let me ask that again. ",
                            "Could you repeat that? ",
                            "I'm sorry, ",
                        ]
                        import random
                        clarification = random.choice(clarifications)
                        return clarification + content
                    else:
                        # Return exact script
                        return content
            else:
                logger.warning("No suitable node found, falling back")
                return await self._process_single_prompt(user_message)
                
        except Exception as e:
            logger.error(f"Error in call flow: {e}")
            return await self._process_single_prompt(user_message)
    
    async def _get_first_conversation_node(self, flow_nodes: list) -> dict:
        """Get the first interactive node in the flow (conversation, collect_input, press_digit, or extract_variable)"""
        for node in flow_nodes:
            if node.get("type") in ["conversation", "collect_input", "press_digit", "extract_variable"]:
                return node
        return None
    
    async def _follow_transition(self, current_node: dict, user_message: str, flow_nodes: list) -> dict:
        """Use AI to evaluate transitions and follow to next node"""
        try:
            node_data = current_node.get("data", {})
            transitions = node_data.get("transitions", [])
            
            if not transitions:
                # No transitions, stay on current node
                logger.info(f"No transitions from {current_node.get('label')}, staying on current node")
                return current_node
            
            # Build options for AI to evaluate
            transition_options = []
            for i, trans in enumerate(transitions):
                condition = trans.get("condition", "")
                next_node_id = trans.get("nextNode", "")
                
                if condition and next_node_id:
                    transition_options.append({
                        "index": i,
                        "condition": condition,
                        "next_node_id": next_node_id
                    })
            
            if not transition_options:
                # No valid transitions, stay on current node
                return current_node
            
            # Ask AI which transition to take
            # Get LLM provider and appropriate client
            llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
            model = self.agent_config.get("model", "gpt-4")
            
            if llm_provider == "grok":
                from server import get_api_key
                grok_key = await get_api_key("grok")
                client = await get_llm_client("grok", api_key=grok_key)
            else:
                client = get_openai_client()
            
            if not client:
                # No AI available, take first transition
                logger.warning("No LLM client available, taking first transition")
                first_trans = transitions[0]
                next_node_id = first_trans.get("nextNode")
                return self._get_node_by_id(next_node_id, flow_nodes) or current_node
            
            # Build evaluation prompt - make it MUCH more intelligent
            options_text = ""
            for i, opt in enumerate(transition_options):
                options_text += f"\nOption {i}:\n"
                options_text += f"  Condition: {opt['condition']}\n"
            
            # Get full conversation context
            full_context = "\n".join([
                f"{msg['role']}: {msg['content']}"
                for msg in self.conversation_history[-5:]  # Last 5 messages for better context
            ])
            
            eval_prompt = f"""You are analyzing a phone conversation to determine which transition path to take based on what the user just said.

CONVERSATION HISTORY:
{full_context}

TRANSITION OPTIONS:
{options_text}

Your task:
1. Carefully read what the user ACTUALLY said in their most recent message
2. Understand the INTENT and MEANING behind their words, not just keywords
3. Consider if they:
   - Are agreeing or saying yes (even if phrased indirectly)
   - Are objecting or have concerns
   - Are asking questions
   - Are saying no time or busy
   - Combination of the above
4. Match their response to the condition that BEST fits their intent

IMPORTANT: 
- Don't just look for exact keyword matches
- Understand natural language variations (e.g., "sure" = agreement, "I guess" = hesitant agreement)
- If they say something like "okay but what is this about?" that's agreement + question
- If they say "no, I'm not interested" that's a clear no
- If they say "I don't have time" or "I'm busy" that matches the time objection

Respond with ONLY the number (0, 1, 2, etc.) of the BEST matching transition.
If absolutely none match, respond with "-1".

Your response (just the number):"""

            logger.info("Evaluating transitions with AI")
            logger.info(f"Options: {len(transition_options)}")
            
            # Call LLM based on provider
            if llm_provider == "grok":
                response = await client.create_completion(
                    messages=[
                        {"role": "system", "content": "You are an expert at understanding conversation flow and user intent in phone calls. You analyze what users say and match it to transition conditions intelligently."},
                        {"role": "user", "content": eval_prompt}
                    ],
                    model=model,
                    temperature=0.1,
                    max_tokens=10
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are an expert at understanding conversation flow and user intent in phone calls. You analyze what users say and match it to transition conditions intelligently."},
                        {"role": "user", "content": eval_prompt}
                    ],
                    temperature=0.1,  # Low temperature for consistent decisions
                    max_tokens=10
                )
            
            ai_response = response.choices[0].message.content.strip()
            logger.info(f"ü§ñ AI transition decision: '{ai_response}'")
            logger.info(f"üìä Available transitions: {[opt['condition'][:50] + '...' for opt in transition_options]}")
            
            try:
                selected_index = int(ai_response)
                logger.info(f"Selected transition index: {selected_index}")
                
                if selected_index >= 0 and selected_index < len(transition_options):
                    next_node_id = transition_options[selected_index]["next_node_id"]
                    next_node = self._get_node_by_id(next_node_id, flow_nodes)
                    
                    if next_node:
                        logger.info(f"Transition: {current_node.get('label')} -> {next_node.get('label')}")
                        return next_node
                    else:
                        logger.warning(f"Next node {next_node_id} not found")
                elif selected_index == -1:
                    # No specific condition matched, look for default/fallback transition
                    logger.info("No specific condition matched, looking for default transition...")
                    
                    # First, look for an explicit default transition (empty condition)
                    for trans in transitions:
                        condition = trans.get("condition", "").strip()
                        next_node_id = trans.get("nextNode", "")
                        
                        # If condition is empty or very generic, it's the default
                        if not condition or condition.lower() in ["default", "otherwise", "else"]:
                            next_node = self._get_node_by_id(next_node_id, flow_nodes)
                            if next_node:
                                logger.info(f"Taking default transition to: {next_node.get('label')}")
                                return next_node
                    
                    # If AI returned -1 (no match), check if node has a GOAL
                    # Goals allow the agent to continue the conversation and guide toward a transition
                    node_goal = node_data.get("goal", "").strip()
                    node_mode = node_data.get("mode", "script")
                    
                    if node_goal:
                        logger.info(f"‚ö†Ô∏è No transition matched, but node has GOAL. Staying on current node to continue with goal-based guidance.")
                        logger.info(f"üéØ Goal: {node_goal[:100]}...")
                        # Return current node - the _process_node_content will use the goal to generate a response
                        # that guides the user toward meeting a transition condition
                        return current_node
                    elif node_mode == "script":
                        # Script mode nodes without goals should stay on current node to re-ask/clarify
                        logger.info(f"‚ö†Ô∏è No transition matched for script-mode node. Staying on current node to clarify/re-ask.")
                        # Return current node - will repeat the script with clarification
                        return current_node
                    
                    # If no goal and not script mode, use FIRST transition as default fallback
                    if transitions and len(transitions) > 0:
                        first_transition = transitions[0]
                        next_node_id = first_transition.get("nextNode")
                        if next_node_id:
                            next_node = self._get_node_by_id(next_node_id, flow_nodes)
                            if next_node:
                                logger.info(f"No condition matched and no goal/script, using first transition as default to: {next_node.get('label')}")
                                return next_node
                    
                    # No transitions or couldn't find next node - stay on current node
                    logger.info("No valid transition found, staying on current node")
                else:
                    logger.info(f"Selected index {selected_index} out of range, staying on current node")
            except ValueError:
                logger.warning(f"Could not parse AI response as integer: '{ai_response}'")
            
            # No transition matched, stay on current node
            logger.info("No transition matched, staying on current node")
            return current_node
            
        except Exception as e:
            logger.error(f"Error following transition: {e}")
            return current_node
    
    def _get_node_by_id(self, node_id: str, flow_nodes: list) -> dict:
        """Find node by ID"""
        for node in flow_nodes:
            if node.get("id") == node_id:
                return node
        return None
    
    async def _extract_variables_realtime(self, extract_variables: list, user_message: str):
        """Extract variables in real-time during conversation"""
        try:
            logger.info(f"üîç Real-time extraction: Processing {len(extract_variables)} variables")
            
            # Only extract variables that don't already exist and aren't empty configs
            variables_to_extract = []
            for var in extract_variables:
                var_name = var.get("name", "")
                if var_name and var_name not in self.session_variables:
                    variables_to_extract.append(var)
                elif var_name:
                    logger.info(f"  ‚úì {var_name}: {self.session_variables[var_name]} (already extracted)")
            
            if not variables_to_extract:
                logger.info("‚úÖ All variables already extracted")
                return
            
            # Build extraction prompt with hints
            extraction_prompt = "Extract the following information from the most recent user message:\n\n"
            for var in variables_to_extract:
                var_name = var.get("name", "")
                var_description = var.get("description", "")
                var_hint = var.get("extraction_hint", "")
                if var_name:
                    extraction_prompt += f"- {var_name}: {var_description}\n"
                    if var_hint:
                        extraction_prompt += f"  Hint: {var_hint}\n"
            
            extraction_prompt += f"\n\nUser's message: {user_message}\n\n"
            extraction_prompt += "Return ONLY a JSON object with the extracted values. If a value cannot be determined from THIS message, use null. Format:\n"
            extraction_prompt += "{"
            for i, var in enumerate(variables_to_extract):
                var_name = var.get("name", "")
                if var_name:
                    extraction_prompt += f'"{var_name}": "extracted_value"'
                    if i < len(variables_to_extract) - 1:
                        extraction_prompt += ", "
            extraction_prompt += "}"
            
            # Call LLM to extract
            client = get_openai_client()
            response = await client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": extraction_prompt}],
                temperature=0.1,
                max_tokens=500
            )
            
            extraction_response = response.choices[0].message.content.strip()
            # Remove markdown if present
            if extraction_response.startswith("```"):
                extraction_response = extraction_response.split("```")[1]
                if extraction_response.startswith("json"):
                    extraction_response = extraction_response[4:]
            extraction_response = extraction_response.strip()
            
            extracted_vars = json.loads(extraction_response)
            
            # Store extracted variables
            for var_name, var_value in extracted_vars.items():
                if var_value is not None:
                    self.session_variables[var_name] = var_value
                    logger.info(f"  ‚úì {var_name}: {var_value} (extracted in real-time)")
            
            logger.info(f"‚úÖ Real-time extraction complete: {len(extracted_vars)} variables")
            
        except Exception as e:
            logger.error(f"Error in real-time extraction: {e}")


    async def _execute_webhook(self, node_data: dict, user_message: str) -> dict:
        """Execute webhook/function call with variable extraction"""
        try:
            webhook_url = node_data.get("webhook_url", "")
            webhook_method = node_data.get("webhook_method", "POST").upper()
            webhook_headers = node_data.get("webhook_headers", {})
            webhook_body_template = node_data.get("webhook_body", "")
            webhook_timeout = node_data.get("webhook_timeout", 10)
            response_variable = node_data.get("response_variable", "webhook_response")
            extract_variables = node_data.get("extract_variables", [])
            
            if not webhook_url:
                logger.warning("No webhook URL configured")
                return {"success": False, "message": "No webhook URL configured"}
            
            # Extract variables from conversation if configured
            if extract_variables and len(extract_variables) > 0:
                logger.info(f"üîç Extracting {len(extract_variables)} variables from conversation...")
                
                # First, check if variables already exist in session_variables
                variables_to_extract = []
                already_available = {}
                
                for var in extract_variables:
                    var_name = var.get("name", "")
                    if var_name:
                        # Check if variable already exists in session
                        if var_name in self.session_variables:
                            already_available[var_name] = self.session_variables[var_name]
                            logger.info(f"  ‚úì {var_name}: {self.session_variables[var_name]} (already in session)")
                        else:
                            variables_to_extract.append(var)
                
                # Only extract variables that aren't already available
                if variables_to_extract:
                    logger.info(f"ü§ñ Need to extract {len(variables_to_extract)} variables from conversation using LLM...")
                    
                    # Build extraction prompt
                    extraction_prompt = "Extract the following information from the conversation:\n\n"
                    for var in variables_to_extract:
                        var_name = var.get("name", "")
                        var_description = var.get("description", "")
                        if var_name:
                            extraction_prompt += f"- {var_name}: {var_description}\n"
                    
                    extraction_prompt += "\n\nConversation history:\n"
                    for msg in self.conversation_history[-10:]:  # Last 10 messages
                        role = msg.get("role", "")
                        content = msg.get("content", "")
                        extraction_prompt += f"{role}: {content}\n"
                    
                    extraction_prompt += f"\nUser's latest message: {user_message}\n\n"
                    extraction_prompt += "Return ONLY a JSON object with the extracted values. If a value cannot be determined, use null. Example format:\n"
                    extraction_prompt += "{"
                    for i, var in enumerate(variables_to_extract):
                        var_name = var.get("name", "")
                        if var_name:
                            extraction_prompt += f'"{var_name}": "extracted_value"'
                            if i < len(variables_to_extract) - 1:
                                extraction_prompt += ", "
                    extraction_prompt += "}"
                    
                    # Call LLM to extract variables
                    try:
                        # Get LLM client
                        llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
                        llm_model = self.agent_config.get("settings", {}).get("llm_model", "gpt-4o-mini")
                        
                        if llm_provider == "openai":
                            client = get_openai_client()
                            response = await client.chat.completions.create(
                                model=llm_model,
                                messages=[{"role": "user", "content": extraction_prompt}],
                                temperature=0.1,
                                max_tokens=500
                            )
                            extraction_response = response.choices[0].message.content
                        else:
                            # Fallback to OpenAI
                            client = get_openai_client()
                            response = await client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": extraction_prompt}],
                                temperature=0.1,
                                max_tokens=500
                            )
                            extraction_response = response.choices[0].message.content
                        
                        # Parse extracted variables
                        extracted_text = extraction_response.strip()
                        # Remove markdown code blocks if present
                        if extracted_text.startswith("```"):
                            extracted_text = extracted_text.split("```")[1]
                            if extracted_text.startswith("json"):
                                extracted_text = extracted_text[4:]
                        extracted_text = extracted_text.strip()
                        
                        extracted_vars = json.loads(extracted_text)
                        
                        # Store extracted variables in session
                        for var_name, var_value in extracted_vars.items():
                            if var_value is not None:
                                self.session_variables[var_name] = var_value
                                logger.info(f"  ‚úì {var_name}: {var_value}")
                        
                        logger.info(f"‚úÖ Extracted {len(extracted_vars)} variables from conversation")
                        
                    except Exception as e:
                        logger.error(f"Error extracting variables: {e}")
                        # Continue anyway with whatever variables we have
                else:
                    logger.info(f"‚úÖ All {len(extract_variables)} variables already available in session")
            
            # Validate mandatory variables
            missing_required = []
            for var in extract_variables:
                var_name = var.get("name", "")
                is_required = var.get("required", False)
                if is_required and var_name:
                    if var_name not in self.session_variables or self.session_variables[var_name] is None:
                        missing_required.append(var)
                        logger.warning(f"‚ùå Required variable missing: {var_name}")
            
            # If mandatory variables are missing, return error with re-prompt
            if missing_required:
                logger.error(f"üö´ Cannot execute webhook: {len(missing_required)} required variables missing")
                
                # Generate re-prompt messages based on type (static or prompt)
                reprompt_messages = []
                for var in missing_required:
                    var_name = var.get("name", "")
                    reprompt_text = var.get("reprompt_text", "")
                    reprompt_type = var.get("reprompt_type", "static")  # Default to static
                    
                    if reprompt_text:
                        if reprompt_type == "prompt":
                            # AI-generated re-prompt based on instruction
                            logger.info(f"ü§ñ Generating AI re-prompt for {var_name} using instruction: {reprompt_text[:50]}...")
                            try:
                                # Build context for AI
                                context_messages = [
                                    {"role": "system", "content": f"You are a helpful phone agent. Generate a natural, conversational response. DO NOT include any format markers or meta-text."},
                                    {"role": "user", "content": f"Instruction: {reprompt_text}\n\nConversation context: {' '.join([msg.get('content', '')[:100] for msg in self.conversation_history[-3:]])}\n\nGenerate a natural response asking for the missing information."}
                                ]
                                
                                client = get_openai_client()
                                response = await client.chat.completions.create(
                                    model="gpt-4o-mini",
                                    messages=context_messages,
                                    temperature=0.7,
                                    max_tokens=150
                                )
                                
                                generated_reprompt = response.choices[0].message.content.strip()
                                reprompt_messages.append(generated_reprompt)
                                logger.info(f"  ‚úì Generated: {generated_reprompt[:50]}...")
                            except Exception as e:
                                logger.error(f"Error generating AI re-prompt: {e}, falling back to instruction text")
                                reprompt_messages.append(reprompt_text)
                        else:
                            # Static re-prompt - use exact text
                            logger.info(f"üì¢ Using static re-prompt for {var_name}: {reprompt_text[:50]}...")
                            reprompt_messages.append(reprompt_text)
                    else:
                        # No re-prompt text provided - generate default
                        var_description = var.get("description", var_name)
                        default_reprompt = f"I need to know {var_description}. Could you provide that?"
                        logger.info(f"üìù Using default re-prompt for {var_name}: {default_reprompt}")
                        reprompt_messages.append(default_reprompt)
                
                # Combine all re-prompts
                combined_reprompt = " ".join(reprompt_messages)
                logger.info(f"üí¨ Final re-prompt message: {combined_reprompt[:100]}...")
                
                return {
                    "success": False,
                    "message": combined_reprompt,
                    "missing_variables": [var.get("name") for var in missing_required],
                    "requires_reprompt": True
                }
            
            logger.info(f"üåê Calling webhook: {webhook_method} {webhook_url}")
            
            # Prepare request body with context
            request_body = {}
            if webhook_body_template:
                try:
                    # Parse the webhook body template to check if it's a schema or a template
                    body_obj = json.loads(webhook_body_template)
                    
                    # Check if it's a JSON schema (has "type" and "properties" keys)
                    if isinstance(body_obj, dict) and "type" in body_obj and "properties" in body_obj:
                        # It's a schema - build request body from extracted variables
                        logger.info("üìã Detected JSON schema format - building request from extracted variables")
                        request_body = {}
                        
                        # Build request body from schema properties and extracted variables
                        for prop_name, prop_def in body_obj.get("properties", {}).items():
                            if prop_name in self.session_variables:
                                request_body[prop_name] = self.session_variables[prop_name]
                            else:
                                # Property not found in extracted variables
                                logger.warning(f"‚ö†Ô∏è Variable '{prop_name}' not found in extracted variables")
                                request_body[prop_name] = None
                    else:
                        # It's a template with {{variable}} placeholders - replace them
                        logger.info("üìù Detected template format - replacing placeholders")
                        body_str = webhook_body_template
                        body_str = body_str.replace("{{user_message}}", user_message)
                        body_str = body_str.replace("{{call_id}}", self.call_id)
                        
                        # Replace session variables
                        for var_name, var_value in self.session_variables.items():
                            body_str = body_str.replace(f"{{{{{var_name}}}}}", str(var_value))
                        
                        request_body = json.loads(body_str)
                except Exception as e:
                    logger.error(f"Error parsing webhook body template: {e}")
                    request_body = {
                        "user_message": user_message,
                        "call_id": self.call_id,
                        "variables": self.session_variables
                    }
            else:
                # Default body
                request_body = {
                    "user_message": user_message,
                    "call_id": self.call_id,
                    "conversation_history": self.conversation_history[-5:],
                    "variables": self.session_variables
                }
            
            logger.info(f"üì§ Request body: {json.dumps(request_body, indent=2)[:500]}...")
            
            # Make HTTP request
            async with httpx.AsyncClient(timeout=webhook_timeout) as client:
                if webhook_method == "GET":
                    response = await client.get(webhook_url, headers=webhook_headers)
                else:  # POST
                    response = await client.post(
                        webhook_url,
                        json=request_body,
                        headers=webhook_headers
                    )
                
                logger.info(f"‚úÖ Webhook response: {response.status_code}")
                
                if response.status_code == 200:
                    # Try to parse JSON response
                    try:
                        response_text = response.text
                        logger.info(f"üì• Response text: {response_text[:200]}...")
                        
                        if response_text.strip():
                            # Try strict JSON parsing first
                            try:
                                response_data = response.json()
                            except json.JSONDecodeError:
                                # If that fails, try with json.loads which is more lenient
                                logger.warning("‚ö†Ô∏è Standard JSON parsing failed, trying lenient parsing...")
                                response_data = json.loads(response_text, strict=False)
                            
                            logger.info(f"üì• Response data: {json.dumps(response_data, indent=2)[:300]}...")
                        else:
                            # Empty response - treat as success
                            logger.warning("‚ö†Ô∏è Webhook returned empty response, treating as success")
                            response_data = {"success": True, "message": "Webhook completed"}
                    except (json.JSONDecodeError, Exception) as e:
                        logger.error(f"‚ùå Failed to parse webhook response as JSON: {e}")
                        logger.error(f"Response text: {response.text[:500]}")
                        # Try to extract JSON from text using regex
                        import re
                        json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
                        if json_match:
                            try:
                                logger.info("üîç Found JSON pattern in response, attempting to parse...")
                                response_data = json.loads(json_match.group(0), strict=False)
                            except:
                                # Return the raw text as response
                                response_data = {
                                    "success": True,
                                    "message": "Webhook completed",
                                    "raw_response": response.text
                                }
                        else:
                            # Return the raw text as response
                            response_data = {
                                "success": True,
                                "message": "Webhook completed",
                                "raw_response": response.text
                            }
                    
                    # Store response in session variables
                    self.session_variables[response_variable] = response_data
                    logger.info(f"üíæ Stored webhook response in variable: {response_variable}")
                    
                    # Extract individual fields from response and update session variables
                    # This allows subsequent nodes to access updated values
                    if isinstance(response_data, dict):
                        logger.info(f"üîÑ Extracting fields from webhook response to update session variables...")
                        logger.info(f"Response data keys: {list(response_data.keys())}")
                        updated_vars = []
                        
                        # Try to intelligently extract data from common n8n/automation response formats
                        actual_data = response_data
                        
                        # Check if we have a raw_response string that might contain JSON
                        if "raw_response" in response_data and isinstance(response_data.get("raw_response"), str):
                            logger.info("üì¶ Detected raw_response string - attempting to parse as JSON...")
                            logger.info(f"raw_response type: {type(response_data['raw_response'])}")
                            logger.info(f"raw_response preview: {str(response_data['raw_response'])[:100]}...")
                            raw_str = response_data["raw_response"]
                            
                            # Check if this contains tool_calls_results with markdown JSON
                            # Many n8n webhooks return JSON embedded in markdown code blocks
                            import re
                            if "tool_calls_results" in raw_str and "```json" in raw_str:
                                logger.info("üì¶ Detected tool_calls_results with markdown JSON - extracting...")
                                try:
                                    # Extract JSON directly from markdown code block (simplest and most reliable)
                                    json_match = re.search(r'```json\s*(\{.*?\})\s*```', raw_str, re.DOTALL)
                                    if json_match:
                                        json_str = json_match.group(1)
                                        actual_data = json.loads(json_str)
                                        logger.info(f"‚úÖ Extracted JSON from markdown: {json.dumps(actual_data, indent=2)[:200]}...")
                                    else:
                                        logger.warning("‚ö†Ô∏è Could not find JSON in markdown code block")
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è Could not extract from markdown: {e}")
                            elif "tool_calls_results" in raw_str:
                                # tool_calls_results exists but no markdown, try standard extraction
                                logger.info("üì¶ Detected tool_calls_results without markdown - trying standard extraction...")
                                try:
                                    temp_data = json.loads(raw_str, strict=False)
                                    result_str = temp_data.get("tool_calls_results", [{}])[0].get("result", "")
                                    if result_str.strip().startswith('{'):
                                        actual_data = json.loads(result_str.strip())
                                        logger.info(f"‚úÖ Parsed result as JSON: {json.dumps(actual_data, indent=2)[:200]}...")
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è Could not extract from tool_calls_results: {e}")
                            else:
                                # No tool_calls_results, try standard JSON parsing
                                try:
                                    actual_data = json.loads(raw_str, strict=False)
                                    logger.info(f"‚úÖ Parsed raw_response as JSON: {list(actual_data.keys())}")
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è Could not parse raw_response as JSON: {e}")
                                    # Try regex extraction
                                    json_match = re.search(r'\{.*\}', raw_str, re.DOTALL)
                                    if json_match:
                                        try:
                                            actual_data = json.loads(json_match.group(0), strict=False)
                                            logger.info(f"‚úÖ Extracted and parsed JSON from raw_response: {list(actual_data.keys())}")
                                        except Exception as e2:
                                            logger.warning(f"‚ö†Ô∏è Regex extraction also failed: {e2}, using original data")
                        
                        # Check for n8n tool_call_response format (if not already handled above)
                        elif "tool_calls_results" in actual_data:
                            logger.info("üì¶ Detected n8n tool_calls_results format - extracting nested data...")
                            try:
                                # Get the result from tool_calls_results array
                                result_str = actual_data.get("tool_calls_results", [{}])[0].get("result", "")
                                
                                # Try to extract JSON from the result string
                                import re
                                json_match = re.search(r'```json\s*(\{.*?\})\s*```', result_str, re.DOTALL)
                                if json_match:
                                    json_str = json_match.group(1)
                                    actual_data = json.loads(json_str)
                                    logger.info(f"‚úÖ Extracted JSON from tool_calls_results: {json.dumps(actual_data, indent=2)[:200]}...")
                                elif result_str.strip().startswith('{'):
                                    # Try parsing as direct JSON
                                    actual_data = json.loads(result_str)
                                    logger.info(f"‚úÖ Parsed result as JSON: {json.dumps(actual_data, indent=2)[:200]}...")
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Could not extract nested JSON: {e}, using top-level data")
                                actual_data = response_data
                        
                        # Check for common wrapper formats
                        if "data" in actual_data and isinstance(actual_data.get("data"), dict):
                            logger.info("üì¶ Detected 'data' wrapper - extracting nested data...")
                            actual_data = actual_data["data"]
                        elif "result" in actual_data and isinstance(actual_data.get("result"), dict):
                            logger.info("üì¶ Detected 'result' wrapper - extracting nested data...")
                            actual_data = actual_data["result"]
                        
                        # Now extract fields from actual_data
                        for field_name, field_value in actual_data.items():
                            # Skip meta fields that shouldn't be individual variables
                            if field_name not in ["success", "message", "error", "status", "response_type", "tool_calls_results", "raw_response"]:
                                self.session_variables[field_name] = field_value
                                updated_vars.append(f"{field_name}={field_value}")
                                logger.info(f"  ‚úì Updated session variable: {field_name} = {field_value}")
                        
                        if updated_vars:
                            logger.info(f"‚úÖ Updated {len(updated_vars)} session variables from webhook response")
                        else:
                            logger.info("‚ÑπÔ∏è No individual fields to extract from webhook response")
                    
                    return {
                        "success": True,
                        "message": "Webhook executed successfully",
                        "data": response_data
                    }
                else:
                    logger.error(f"Webhook failed with status {response.status_code}: {response.text}")
                    return {
                        "success": False,
                        "message": f"Webhook failed with status {response.status_code}"
                    }
                    
        except httpx.TimeoutException:
            logger.error(f"Webhook timeout after {webhook_timeout}s")
            return {"success": False, "message": "Webhook timeout"}
        except Exception as e:
            logger.error(f"Error executing webhook: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"success": False, "message": f"Webhook error: {str(e)}"}
    
    def _handle_transfer(self, node_data: dict) -> dict:
        """Handle call transfer request"""
        try:
            transfer_type = node_data.get("transfer_type", "cold")  # cold or warm
            destination_type = node_data.get("destination_type", "phone")  # phone or agent
            destination = node_data.get("destination", "")
            transfer_message = node_data.get("transfer_message", "Please hold while I transfer your call...")
            
            logger.info(f"üìû Transfer request: {transfer_type} transfer to {destination_type}: {destination}")
            
            # In a real implementation, this would:
            # 1. For phone: Initiate SIP/PSTN transfer to phone number
            # 2. For agent: Transfer to another agent or queue
            # 3. Handle warm transfer (conference) vs cold transfer (blind)
            
            # For now, we store the transfer request and return info
            return {
                "success": True,
                "transfer_type": transfer_type,
                "destination_type": destination_type,
                "destination": destination,
                "message": transfer_message
            }
            
        except Exception as e:
            logger.error(f"Error handling transfer: {e}")
            return {
                "success": False,
                "message": "Transfer failed. Please stay on the line."
            }
    
    def _handle_collect_input(self, node_data: dict, user_input: str) -> dict:
        """Validate and collect user input based on specified type"""
        try:
            import re
            
            input_type = node_data.get("input_type", "text")  # text, email, phone, number
            variable_name = node_data.get("variable_name", "user_input")
            error_message = node_data.get("error_message", "That doesn't look right. Please try again.")
            
            logger.info(f"üìù Validating input type: {input_type}")
            
            # Validate based on input type
            if input_type == "email":
                # Basic email validation
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                if re.match(email_pattern, user_input.strip()):
                    return {
                        "valid": True,
                        "value": user_input.strip(),
                        "success_message": f"Got it, your email is {user_input.strip()}."
                    }
                else:
                    return {
                        "valid": False,
                        "error": "Invalid email format",
                        "error_message": error_message or "That doesn't look like a valid email address. Please provide your email."
                    }
            
            elif input_type == "phone":
                # Basic phone validation (digits only, 10-15 digits)
                phone_digits = re.sub(r'[^\d]', '', user_input)
                if 10 <= len(phone_digits) <= 15:
                    return {
                        "valid": True,
                        "value": phone_digits,
                        "success_message": "Thank you, I have your phone number."
                    }
                else:
                    return {
                        "valid": False,
                        "error": "Invalid phone format",
                        "error_message": error_message or "That doesn't look like a valid phone number. Please provide your phone number."
                    }
            
            elif input_type == "number":
                # Validate number
                try:
                    number_value = float(user_input.strip())
                    return {
                        "valid": True,
                        "value": number_value,
                        "success_message": f"Got it, {number_value}."
                    }
                except ValueError:
                    return {
                        "valid": False,
                        "error": "Not a valid number",
                        "error_message": error_message or "Please provide a valid number."
                    }
            
            else:  # text - accept any non-empty input
                if user_input.strip():
                    return {
                        "valid": True,
                        "value": user_input.strip(),
                        "success_message": "Thank you."
                    }
                else:
                    return {
                        "valid": False,
                        "error": "Empty input",
                        "error_message": error_message or "Please provide your information."
                    }
                    
        except Exception as e:
            logger.error(f"Error validating input: {e}")
            return {
                "valid": False,
                "error": str(e),
                "error_message": "I didn't understand that. Please try again."
            }
    
    async def _handle_send_sms(self, node_data: dict, user_message: str) -> dict:
        """Send SMS message (mock implementation)"""
        try:
            phone_number = node_data.get("phone_number", "")
            sms_message = node_data.get("sms_message", "")
            
            # Replace variables in phone number and message
            if phone_number:
                for var_name, var_value in self.session_variables.items():
                    phone_number = phone_number.replace(f"{{{{{var_name}}}}}", str(var_value))
            
            if sms_message:
                for var_name, var_value in self.session_variables.items():
                    sms_message = sms_message.replace(f"{{{{{var_name}}}}}", str(var_value))
            
            logger.info(f"üì± Sending SMS to {phone_number}: {sms_message[:50]}...")
            
            # In a real implementation, this would:
            # 1. Use Twilio, AWS SNS, or similar SMS service
            # 2. Send actual SMS to phone number
            # 3. Handle delivery status and callbacks
            
            # For now, we mock the SMS sending
            if not phone_number or not sms_message:
                return {
                    "success": False,
                    "status": "failed",
                    "error": "Missing phone number or message",
                    "message": "I couldn't send the SMS. Missing information."
                }
            
            # Mock successful SMS
            logger.info(f"‚úÖ SMS sent successfully to {phone_number}")
            return {
                "success": True,
                "status": "sent",
                "phone_number": phone_number,
                "message": "I've sent you an SMS with the information."
            }
            
        except Exception as e:
            logger.error(f"Error sending SMS: {e}")
            return {
                "success": False,
                "status": "error",
                "error": str(e),
                "message": "I encountered an error sending the SMS. Please try again."
            }
    
    def _evaluate_logic_conditions(self, node_data: dict) -> str:
        """Evaluate logic conditions and return next node ID"""
        try:
            conditions = node_data.get("conditions", [])
            
            logger.info(f"üîÄ Evaluating {len(conditions)} conditions")
            
            for idx, condition in enumerate(conditions):
                variable_name = condition.get("variable", "")
                operator = condition.get("operator", "equals")
                compare_value = condition.get("value", "")
                next_node_id = condition.get("nextNode", "")
                
                # Get variable value from session
                variable_value = self.session_variables.get(variable_name, "")
                
                logger.info(f"  Condition {idx + 1}: {variable_name} {operator} {compare_value}")
                logger.info(f"    Variable value: {variable_value}")
                
                # Evaluate condition
                result = False
                
                if operator == "equals":
                    result = str(variable_value).lower() == str(compare_value).lower()
                elif operator == "not_equals":
                    result = str(variable_value).lower() != str(compare_value).lower()
                elif operator == "contains":
                    result = str(compare_value).lower() in str(variable_value).lower()
                elif operator == "greater_than":
                    try:
                        result = float(variable_value) > float(compare_value)
                    except (ValueError, TypeError):
                        result = False
                elif operator == "less_than":
                    try:
                        result = float(variable_value) < float(compare_value)
                    except (ValueError, TypeError):
                        result = False
                elif operator == "exists":
                    result = variable_name in self.session_variables and variable_value not in [None, "", "undefined"]
                elif operator == "not_exists":
                    result = variable_name not in self.session_variables or variable_value in [None, "", "undefined"]
                
                logger.info(f"    Result: {result}")
                
                if result and next_node_id:
                    logger.info(f"‚úÖ Condition {idx + 1} matched - routing to node {next_node_id}")
                    return next_node_id
            
            # No condition matched - use default path
            default_next_node = node_data.get("default_next_node", "")
            if default_next_node:
                logger.info(f"‚ÑπÔ∏è No conditions matched - using default path: {default_next_node}")
                return default_next_node
            
            logger.info("‚ö†Ô∏è No conditions matched and no default path")
            return None
            
        except Exception as e:
            logger.error(f"Error evaluating logic conditions: {e}")
            return None
    
    def _handle_press_digit(self, node_data: dict, user_message: str) -> dict:
        """Handle DTMF digit press"""
        try:
            # Extract digit from user message (looking for single digit 0-9, *, #)
            import re
            
            # Check if message contains a single digit or DTMF character
            digit_match = re.search(r'[0-9*#]', user_message)
            
            if digit_match:
                digit = digit_match.group(0)
                logger.info(f"üî¢ Detected digit: {digit}")
                
                # Get digit mappings from node data
                digit_mappings = node_data.get("digit_mappings", {})
                
                # Find corresponding next node for this digit
                next_node_id = digit_mappings.get(digit, "")
                
                if next_node_id:
                    return {
                        "digit": digit,
                        "next_node_id": next_node_id,
                        "message": f"You pressed {digit}."
                    }
                else:
                    return {
                        "digit": digit,
                        "next_node_id": None,
                        "message": f"You pressed {digit}, but no action is configured for this digit."
                    }
            else:
                prompt = node_data.get("prompt_message", "Please press a digit from 0 to 9, * or #.")
                return {
                    "digit": None,
                    "next_node_id": None,
                    "message": prompt
                }
                
        except Exception as e:
            logger.error(f"Error handling press digit: {e}")
            return {
                "digit": None,
                "next_node_id": None,
                "message": "Please press a digit."
            }
    
    async def _handle_extract_variable(self, node_data: dict, user_message: str) -> dict:
        """Extract variable from user message using AI"""
        try:
            variable_name = node_data.get("variable_name", "extracted_data")
            extraction_prompt = node_data.get("extraction_prompt", "")
            
            logger.info(f"üìã Extracting {variable_name} from: {user_message}")
            
            # Use AI to extract the variable
            client = get_openai_client()
            if not client:
                logger.warning("OpenAI client not available for extraction")
                return {
                    "success": False,
                    "message": "Could not extract information."
                }
            
            # Create extraction prompt
            system_prompt = f"""Extract the following information from the user's message: {extraction_prompt}

Return ONLY the extracted value, nothing else. If you cannot find the information, respond with "NOT_FOUND".

Examples:
- If asked to extract a name from "My name is John", return: John
- If asked to extract an email from "Contact me at john@email.com", return: john@email.com
- If asked to extract a date from "I'll be there on Monday", return: Monday"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            response = await client.chat.completions.create(
                model=self.agent_config.get("model", "gpt-4-turbo"),
                messages=messages,
                temperature=0.3,
                max_tokens=100
            )
            
            extracted_value = response.choices[0].message.content.strip()
            
            if extracted_value and extracted_value != "NOT_FOUND":
                logger.info(f"‚úÖ Extracted value: {extracted_value}")
                return {
                    "success": True,
                    "value": extracted_value,
                    "message": f"Got it, {extracted_value}."
                }
            else:
                logger.info(f"‚ùå Could not extract {variable_name}")
                return {
                    "success": False,
                    "message": f"I couldn't find the {variable_name}. Could you please provide it again?"
                }
                
        except Exception as e:
            logger.error(f"Error extracting variable: {e}")
            return {
                "success": False,
                "message": "I had trouble understanding. Could you please repeat that?"
            }
    
    async def _generate_ai_response_streaming(self, content: str, stream_callback=None) -> str:
        """Generate AI response with streaming support"""
        import re
        
        # Get LLM provider and appropriate client
        llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
        model = self.agent_config.get("model", "gpt-4-turbo")
        
        if llm_provider == "grok":
            from server import get_api_key
            grok_key = await get_api_key("grok")
            client = await get_llm_client("grok", api_key=grok_key)
        else:
            client = get_openai_client()
        
        if not client:
            # No AI available, return content as-is
            if stream_callback:
                await stream_callback(content)
            return content
        
        # Extract actual script from content if it has markers
        script_to_use = content
        extracted = False
        if "AGENT_SCRIPT_LINE_INPUT:" in content:
            # Extract the script line from format like: `AGENT_SCRIPT_LINE_INPUT:` "{{customer_name}}?"
            match = re.search(r'AGENT_SCRIPT_LINE_INPUT[:`\s]+["\']([^"\']+)["\']', content)
            if match:
                script_to_use = match.group(1)
                extracted = True
                logger.info(f"üìù Extracted script: {script_to_use}")
        
        # If we extracted the script, just return it with variable substitution
        if extracted:
            # Do variable substitution
            for var_name, var_value in self.session_variables.items():
                placeholder = f"{{{{{var_name}}}}}"
                script_to_use = script_to_use.replace(placeholder, str(var_value))
            
            if stream_callback:
                await stream_callback(script_to_use)
                logger.info(f"üì§ Streamed extracted script: {script_to_use[:50]}...")
            return script_to_use
        
        # Otherwise, use LLM to interpret the prompt with streaming
        # üöÄ GROK PREFIX CACHING: Reuse the EXACT same system prompt string object
        # Grok automatically caches based on prefix matching, so using the same string enables caching
        
        # STATIC PART (cached across all turns) - built once in __init__, reused every turn
        # This enables Grok's automatic prefix caching to work efficiently
        cached_system_prompt = self._cached_system_prompt
        
        # SMART ROUTING: Determine if KB retrieval is needed
        kb_context = ""
        if self.knowledge_base and self.agent_id:
            from kb_router import needs_knowledge_base
            from rag_service import retrieve_relevant_chunks
            
            # Get last user message for routing decision
            last_user_msg = ""
            for msg in reversed(self.conversation_history):
                if msg.get("role") == "user":
                    last_user_msg = msg.get("content", "")
                    break
            
            needs_kb, reason = needs_knowledge_base(last_user_msg)
            
            if needs_kb:
                # Factual question - retrieve relevant KB chunks
                try:
                    retrieval_start = time.time()
                    kb_context = retrieve_relevant_chunks(
                        self.agent_id, 
                        last_user_msg, 
                        top_k=3,  # Reduced from 5 for speed
                        use_cache=True
                    )
                    retrieval_time = (time.time() - retrieval_start) * 1000
                    logger.info(f"üîç RAG retrieval: {retrieval_time:.0f}ms")
                    
                    if kb_context:
                        # Add natural instruction to acknowledge before answering (masks retrieval latency)
                        kb_context = f"\n\n=== RELEVANT KNOWLEDGE ===\n{kb_context}\n‚ö†Ô∏è INSTRUCTIONS:\n- Use ONLY this information. Do NOT make up facts.\n- Start with a natural acknowledgment (e.g., 'Great question...', 'Sure...', 'Let me tell you...') before answering\n- Vary your acknowledgments - don't use the same phrase repeatedly\n=== END KNOWLEDGE ===\n"
                except Exception as e:
                    logger.error(f"‚ùå RAG retrieval error: {e}")
            else:
                logger.info(f"‚ö° Skipping KB (reason: {reason}) - fast path enabled")
        
        # DYNAMIC PART (changes every turn) - context, customer info, current instruction
        # KB added here ONLY when needed (smart routing)
        dynamic_context = f"""
# CURRENT CONTEXT
Customer: {self.session_variables.get('customer_name', 'the customer')}
Variables: {json.dumps(self.session_variables)}
{kb_context}
# YOUR TASK RIGHT NOW
{content}

Respond naturally to the user based on these instructions. Remember: DO NOT repeat what you've already said."""
        
        # Construct messages with caching hint
        # IMPORTANT: With Grok's 2M token context, include FULL conversation history
        # This ensures the LLM never loses context and can see everything that was said
        # KB is cached in the system prompt, so it doesn't add latency on subsequent turns
        messages = [
            {"role": "system", "content": cached_system_prompt, "cache_control": {"type": "ephemeral"}},
            {"role": "system", "content": dynamic_context}
        ] + self.conversation_history  # FULL history, not limited
        
        # Log context usage
        history_count = len(self.conversation_history)
        total_prompt_chars = len(cached_system_prompt) + len(dynamic_context)
        logger.info(f"üí¨ Sending to LLM: {history_count} conversation turns, {total_prompt_chars} system chars (KB cached)")
        
        # Stream LLM response and process sentence by sentence
        llm_request_start = time.time()
        
        # Call LLM based on provider with streaming
        if llm_provider == "grok":
            response = await client.create_completion(
                messages=messages,
                model=model,
                temperature=0.7,
                max_tokens=300,
                stream=True
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=300,
                stream=True
            )
        
        # Collect response and stream sentences
        full_response = ""
        sentence_buffer = ""
        first_token_received = False
        
        # Sentence delimiters
        sentence_endings = re.compile(r'([.!?]\s+)')
        
        async for chunk in response:
            if not first_token_received:
                ttft_ms = int((time.time() - llm_request_start) * 1000)
                logger.info(f"‚è±Ô∏è  LLM TTFT: {ttft_ms}ms ({llm_provider} {model})")
                first_token_received = True
            
            # Extract content from chunk
            if llm_provider == "grok":
                if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        chunk_content = delta.content
                    else:
                        continue
                else:
                    continue
            else:
                if chunk.choices[0].delta.content:
                    chunk_content = chunk.choices[0].delta.content
                else:
                    continue
            
            full_response += chunk_content
            sentence_buffer += chunk_content
            
            # Check if we have a complete sentence
            if sentence_endings.search(sentence_buffer):
                # Split into sentences
                sentences = sentence_endings.split(sentence_buffer)
                
                # Process complete sentences (leave last incomplete one in buffer)
                for i in range(0, len(sentences) - 1, 2):
                    if i < len(sentences):
                        sentence = sentences[i]
                        if i + 1 < len(sentences):
                            sentence += sentences[i + 1]  # Add delimiter
                        
                        sentence = sentence.strip()
                        if sentence and stream_callback:
                            # Stream this sentence immediately to TTS
                            await stream_callback(sentence)
                            logger.info(f"üì§ Streamed sentence: {sentence[:50]}...")
                
                # Keep the last incomplete part in buffer
                sentence_buffer = sentences[-1] if len(sentences) % 2 != 0 else ""
        
        # Send any remaining text
        if sentence_buffer.strip() and stream_callback:
            await stream_callback(sentence_buffer.strip())
            logger.info(f"üì§ Streamed final fragment: {sentence_buffer[:50]}...")
        
        return full_response
    
    async def _process_node_content_streaming(self, node: dict, user_message: str, flow_nodes: list, stream_callback=None) -> str:
        """Process a node and return its content/response with streaming support"""
        node_data = node.get("data", {})
        node_type = node.get("type", "")
        
        # Get the actual script content properly
        if node_type == "conversation":
            content = node_data.get("script", "") or node_data.get("content", "")
        elif node_type == "ending":
            content = node_data.get("script", "") or node_data.get("content", "")
        else:
            content = node_data.get("content", "") or node_data.get("script", "")
        
        # Get mode from data (can be "prompt" or "script")
        # Check both "mode" and "promptType" fields
        prompt_type = node_data.get("mode")
        if prompt_type is None:
            prompt_type = node_data.get("promptType")
        
        # If still None, auto-detect based on content
        if prompt_type is None:
            # If content is very long (>500 chars) or contains instruction markers, treat as prompt
            if len(content) > 500 or any(marker in content.lower() for marker in [
                "## ", "### ", "instructions:", "goal:", "objective:", "**important**", 
                "you are", "your task", "rules:", "your role", "context:", "important:"
            ]):
                prompt_type = "prompt"
                logger.info(f"üîç Auto-detected PROMPT mode in _process_node_content_streaming (length: {len(content)} chars)")
            else:
                prompt_type = "script"
                logger.info(f"üîç Auto-detected SCRIPT mode in _process_node_content_streaming (length: {len(content)} chars)")
        
        logger.info(f"üìã Processing node streaming - Type: {node_type}, Mode: {prompt_type}, Content length: {len(content)} chars")
        
        # Handle different node types
        if node_type == "ending":
            self.should_end_call = True
            if stream_callback:
                await stream_callback(content)
                logger.info(f"üì§ Streamed ending content: {content[:50]}...")
            return content
        
        if node_type == "logic_split":
            # Evaluate conditions and route to next node
            next_node_id = self._evaluate_logic_conditions(node_data)
            if next_node_id:
                next_node = self._get_node_by_id(next_node_id, flow_nodes)
                if next_node:
                    self.current_node_id = next_node_id
                    return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
            # No condition matched - return default message
            return "Let me help you with that."
        
        if node_type == "function":
            webhook_response = await self._execute_webhook(node_data, user_message)
            # Continue to next node after webhook
            if node_data.get("transitions"):
                next_node = await self._follow_transition(node, user_message, flow_nodes)
                if next_node and next_node.get("id") != node.get("id"):
                    return await self._process_node_content_streaming(next_node, user_message, flow_nodes, stream_callback)
            return webhook_response.get("message", "Function executed")
        
        # Handle script vs prompt mode for conversation nodes
        if prompt_type == "script":
            logger.info("üìú Using SCRIPT mode - will speak content directly")
            if stream_callback:
                await stream_callback(content)
                logger.info(f"üì§ Streamed script content: {content[:50]}...")
            return content
        else:
            # Prompt mode - use AI with streaming
            logger.info("ü§ñ Using PROMPT mode - will generate AI response")
            return await self._generate_ai_response_streaming(content, stream_callback)
    
    async def _process_node_content(self, node: dict, user_message: str, flow_nodes: list) -> str:
        """Process a node and return its content/response"""
        node_data = node.get("data", {})
        node_type = node.get("type", "")
        
        # Get the actual script content properly
        if node_type == "conversation":
            content = node_data.get("script", "") or node_data.get("content", "")
        elif node_type == "ending":
            content = node_data.get("script", "") or node_data.get("content", "")
        else:
            content = node_data.get("content", "") or node_data.get("script", "")
        
        # Get mode from data (can be "prompt" or "script")
        # Check both "mode" and "promptType" fields
        prompt_type = node_data.get("mode")
        if prompt_type is None:
            prompt_type = node_data.get("promptType")
        
        # If still None, auto-detect based on content
        if prompt_type is None:
            # If content is very long (>500 chars) or contains instruction markers, treat as prompt
            if len(content) > 500 or any(marker in content.lower() for marker in [
                "## ", "### ", "instructions:", "goal:", "objective:", "**important**", 
                "you are", "your task", "rules:", "your role", "context:", "important:"
            ]):
                prompt_type = "prompt"
                logger.info(f"üîç Auto-detected PROMPT mode in _process_node_content (length: {len(content)} chars)")
            else:
                prompt_type = "script"
                logger.info(f"üîç Auto-detected SCRIPT mode in _process_node_content (length: {len(content)} chars)")
        
        logger.info(f"üìã Processing node - Type: {node_type}, Mode: {prompt_type}, Content length: {len(content)} chars")
        
        # Handle different node types
        if node_type == "ending":
            self.should_end_call = True
            return content
        
        if node_type == "logic_split":
            # Evaluate conditions and route to next node
            next_node_id = self._evaluate_logic_conditions(node_data)
            if next_node_id:
                next_node = self._get_node_by_id(next_node_id, flow_nodes)
                if next_node:
                    self.current_node_id = next_node_id
                    return await self._process_node_content(next_node, user_message, flow_nodes)
            # No condition matched - return default message
            return "Let me help you with that."
        
        if node_type == "function":
            webhook_response = await self._execute_webhook(node_data, user_message)
            # Continue to next node after webhook
            if node_data.get("transitions"):
                next_node = await self._follow_transition(node, user_message, flow_nodes)
                if next_node and next_node.get("id") != node.get("id"):
                    return await self._process_node_content(next_node, user_message, flow_nodes)
            return webhook_response.get("message", "Function executed")
        
        # Handle script vs prompt mode for conversation nodes
        if prompt_type == "script":
            logger.info("üìú Using SCRIPT mode - will speak content directly")
            # Replace variables in script
            script = content
            logger.info(f"üîß Script before replacement: {script[:100]}")
            logger.info(f"üîß Session variables: {self.session_variables}")
            for var_name, var_value in self.session_variables.items():
                logger.info(f"üîß Replacing {{{{{var_name}}}}} with {var_value}")
                script = script.replace(f"{{{{{var_name}}}}}", str(var_value))
            logger.info(f"üîß Script after replacement: {script[:100]}")
            return script
        else:
            logger.info("üí≠ Using PROMPT mode - will use AI to interpret instructions")
            # Prompt mode - use AI
            # FIRST replace variables in the content/prompt itself
            prompt_with_vars = content
            logger.info(f"üîß Prompt before replacement: {prompt_with_vars[:100]}")
            logger.info(f"üîß Session variables: {self.session_variables}")
            for var_name, var_value in self.session_variables.items():
                logger.info(f"üîß Replacing {{{{{var_name}}}}} with {var_value}")
                prompt_with_vars = prompt_with_vars.replace(f"{{{{{var_name}}}}}", str(var_value))
            logger.info(f"üîß Prompt after replacement: {prompt_with_vars[:100]}")
            logger.info(f"üìè Full prompt length: {len(prompt_with_vars)} chars")
            
            # Get LLM provider from agent settings
            llm_provider = self.agent_config.get("settings", {}).get("llm_provider", "openai")
            model = self.agent_config.get("model", "gpt-4-turbo")
            
            # Validate model matches provider and fix if mismatched
            grok_models = ["grok-4-fast-non-reasoning", "grok-4-fast-reasoning", "grok-3", "grok-2-1212", "grok-beta", "grok-4-fast"]
            openai_models = ["gpt-4.1-2025-04-14", "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo"]
            
            if llm_provider == "grok":
                if model not in grok_models:
                    logger.warning(f"‚ö†Ô∏è  Model '{model}' not valid for Grok, using 'grok-3'")
                    model = "grok-3"
            else:
                if model in grok_models:
                    logger.warning(f"‚ö†Ô∏è  Model '{model}' is a Grok model but provider is OpenAI, using 'gpt-4-turbo'")
                    model = "gpt-4-turbo"
            
            logger.info(f"ü§ñ Using LLM provider: {llm_provider}, model: {model}")
            
            # Get appropriate client
            if llm_provider == "grok":
                from server import get_api_key
                grok_key = await get_api_key("grok")
                client = await get_llm_client("grok", api_key=grok_key)
            else:
                client = await get_llm_client("openai")
                
            if not client:
                logger.warning(f"‚ö†Ô∏è {llm_provider} client not available, returning raw prompt")
                return prompt_with_vars
            
            # Include session variables in system prompt
            # üöÄ HYBRID CACHING: Separate static (cached) and dynamic (per-turn) parts
            
            # GLOBAL PROMPT - Universal personality and behavior layer
            global_prompt = self.agent_config.get("system_prompt", "").strip()
            
            # Add knowledge base to global prompt if available
            if self.knowledge_base:
                global_prompt += f"\n\n=== KNOWLEDGE BASE ===\nYou have access to multiple reference sources below. Each source serves a different purpose.\n\nüß† HOW TO USE THE KNOWLEDGE BASE:\n1. When user asks a question, FIRST identify which knowledge base source(s) are relevant based on their descriptions\n2. Read ONLY the relevant source(s) to find the answer\n3. Use ONLY information from the knowledge base - do NOT make up or improvise ANY factual details\n4. If the knowledge base doesn't contain the answer, say: \"I don't have that specific information available\"\n5. Different sources contain different types of information - match the user's question to the right source\n\n‚ö†Ô∏è NEVER invent: company names, product names, prices, processes, methodologies, or any factual information not in the knowledge base\n\n{self.knowledge_base}\n=== END KNOWLEDGE BASE ===\n"
            
            if global_prompt:
                # User has defined a custom global prompt - use it as the foundation
                logger.info(f"üìã Using custom global prompt ({len(global_prompt)} chars)")
                cached_system_prompt = global_prompt
            else:
                # No global prompt defined - use default conversational AI behavior
                logger.info("üìã Using default system prompt (no global prompt configured)")
                cached_system_prompt = """You are a conversational AI assistant having real conversations with users.

# COMMUNICATION STYLE  
- Generate natural, conversational responses
- Be concise but complete
- Maintain context across the conversation
- Sound like a real person, not a robot

# STRICT RULES
- RESPOND to the user, don't just analyze instructions
- NO meta-commentary about what you're doing
- Keep it natural and flowing"""
            
            # DYNAMIC PART (changes every turn) - current context, variables, specific instructions
            var_context = ""
            if self.session_variables:
                var_context = f"\n\nAvailable context variables: {json.dumps(self.session_variables)}"
            
            dynamic_context = f"""
# CURRENT TASK
Your instructions for this conversation:
{prompt_with_vars}{var_context}

IMPORTANT: Based on these instructions, generate a natural, conversational response to the user."""
            
            messages = [
                {"role": "system", "content": cached_system_prompt, "cache_control": {"type": "ephemeral"}},
                {"role": "system", "content": dynamic_context}
            ] + self.conversation_history  # Full history
            
            logger.info(f"ü§ñ Sending to {llm_provider} - Model: {model}")
            logger.info(f"ü§ñ System message length: {len(cached_system_prompt) + len(dynamic_context)} chars")
            logger.info(f"ü§ñ Conversation history: {len(self.conversation_history)} messages (sending last 5)")
            
            # Call LLM based on provider
            if llm_provider == "grok":
                response = await client.create_completion(
                    messages=messages,
                    model=model,
                    temperature=0.7,
                    max_tokens=500
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=500
            )
            
            ai_response = response.choices[0].message.content
            logger.info(f"‚úÖ AI response received: {ai_response[:200]}...")
            logger.info(f"üìè AI response length: {len(ai_response)} chars")
            
            if not ai_response or ai_response.strip() == "":
                logger.error("‚ùå AI returned EMPTY response!")
                return "I apologize, I'm having trouble formulating a response. Could you please repeat that?"
            
            return ai_response

    async def _select_node_with_ai(self, user_message: str, flow_nodes: list) -> dict:
        """Use AI to select the best node based on transitions"""
        try:
            # Get conversation nodes with transitions
            available_nodes = [n for n in flow_nodes if n.get("type") == "conversation"]
            
            if not available_nodes:
                return None
            
            # Build options for AI to choose from
            node_options = []
            for i, node in enumerate(available_nodes):
                node_data = node.get("data", {})
                transitions = node_data.get("transitions", [])
                
                # Collect all transition conditions for this node
                conditions = []
                for trans in transitions:
                    condition = trans.get("condition", "")
                    if condition:
                        conditions.append(condition)
                
                if conditions:
                    node_options.append({
                        "index": i,
                        "label": node.get("label", "Node"),
                        "conditions": " OR ".join(conditions)
                    })
            
            if not node_options:
                # No transitions defined, return first conversation node
                return available_nodes[0]
            
            # Ask AI to select best node
            client = get_openai_client()
            if not client:
                return available_nodes[0]
            
            # Build selection prompt
            options_text = "\n".join([
                f"{opt['index']}: {opt['label']} - Use if: {opt['conditions']}"
                for opt in node_options
            ])
            
            recent_context = "\n".join([
                f"{msg['role']}: {msg['content']}"
                for msg in self.conversation_history[-3:]
            ])
            
            selection_prompt = f"""Based on the conversation context, select which node to use.

Recent conversation:
{recent_context}

Available nodes:
{options_text}

Respond with ONLY the number of the best matching node (e.g., "0" or "1" or "2").
If none match well, respond with "0"."""

            response = await client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": selection_prompt}],
                temperature=0.3,
                max_tokens=10
            )
            
            selected_index = int(response.choices[0].message.content.strip())
            logger.info(f"AI selected node index: {selected_index}")
            
            return available_nodes[selected_index] if selected_index < len(available_nodes) else available_nodes[0]
            
        except Exception as e:
            logger.error(f"Error in AI node selection: {e}")
            # Fallback to first conversation node
            for node in flow_nodes:
                if node.get("type") == "conversation":
                    return node
            return None
    
    async def synthesize_speech(self, text: str):
        """Convert text to speech using ElevenLabs - Uses settings from agent config"""
        try:
            # Get ElevenLabs settings from agent config
            elevenlabs_settings = self.agent_config.get("settings", {}).get("elevenlabs_settings", {})
            
            # Use voice_id from elevenlabs_settings, with fallback to Rachel's voice ID
            voice_id = elevenlabs_settings.get("voice_id", "21m00Tcm4TlvDq8ikWAM")
            model = elevenlabs_settings.get("model", "eleven_turbo_v2_5")
            
            # Log which voice and model we're using
            logger.info(f"üé§ Synthesizing speech with voice_id={voice_id}, model={model}: {text[:50]}...")
            
            # NOTE: This is a legacy function. The actual TTS is handled by telnyx_service.speak_text()
            # which correctly uses all elevenlabs_settings including voice_id, model, speed, stability, etc.
            # This function is kept for compatibility but should eventually be removed.
            
            return {
                "audio": "base64_encoded_audio_data",
                "text": text
            }
            
        except Exception as e:
            logger.error(f"Error synthesizing speech: {e}")
            return None
    
    async def send_audio_chunk(self, audio_data: bytes):
        """Send audio chunk to Deepgram for transcription"""
        try:
            if self.deepgram_connection and self.is_active:
                await self.deepgram_connection.send(audio_data)
        except Exception as e:
            logger.error(f"Error sending audio chunk: {e}")
    
    async def close(self):
        """Close the call session"""
        self.is_active = False
        if self.deepgram_connection:
            await self.deepgram_connection.finish()
        logger.info(f"Call session {self.call_id} closed")


# Global storage for active call sessions
active_sessions: Dict[str, CallSession] = {}


async def create_call_session(call_id: str, agent_config: dict, agent_id: str = None, user_id: str = None, db=None) -> CallSession:
    """Create a new call session with RAG-enabled KB (smart routing + retrieval)"""
    # Extract user_id from agent_config if not provided
    if not user_id:
        user_id = agent_config.get("user_id")
    
    # Check if agent has KB items (will be retrieved dynamically via RAG when needed)
    has_kb = False
    if db is not None and agent_id:
        try:
            kb_count = await db.knowledge_base.count_documents({"agent_id": agent_id})
            if kb_count > 0:
                has_kb = True
                logger.info(f"üìö Agent {agent_id} has {kb_count} KB items - Smart routing enabled (RAG on demand)")
        except Exception as e:
            logger.error(f"Error checking KB for agent {agent_id}: {e}")
    
    # Pass flag to indicate RAG is available (actual retrieval happens per-query)
    knowledge_base = "RAG_ENABLED" if has_kb else ""
    
    session = CallSession(call_id, agent_config, agent_id=agent_id, user_id=user_id, knowledge_base=knowledge_base)
    await session.initialize_deepgram()
    active_sessions[call_id] = session
    return session


async def get_call_session(call_id: str) -> CallSession:
    """Get an existing call session"""
    return active_sessions.get(call_id)


async def close_call_session(call_id: str):
    """Close a call session"""
    session = active_sessions.get(call_id)
    if session:
        await session.close()
        del active_sessions[call_id]
