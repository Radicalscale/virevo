"""
RUNPOD SESAME TTS - TRUE STREAMING IMPLEMENTATION
Based on csm-streaming by davidbrowne17
Achieves <1s latency by streaming audio chunks as they're generated
"""

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import FileResponse
from pydantic import BaseModel
import torch
import logging
import os
import uuid
import json
from pathlib import Path
from huggingface_hub import login

# Import from csm-streaming repo
from generator import load_csm_1b, Segment

app = FastAPI()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create output directory
OUTPUT_DIR = Path("/tmp/sesame_outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# Pre-load model at startup
logger.info("ðŸš€ Pre-loading Sesame CSM-1B with streaming support...")
hf_token = os.getenv("HF_TOKEN")
if hf_token:
    login(hf_token)

# Load model with streaming support
generator = load_csm_1b("cuda")
logger.info(f"âœ… Model loaded with TRUE STREAMING support")


@app.get("/")
async def root():
    return {
        "status": "ok",
        "device": "cuda",
        "model_loaded": True,
        "websocket_endpoint": "/ws/generate",
        "streaming": "true_realtime",
        "rtf": "0.28x"  # Real-time factor on 4090
    }


@app.websocket("/ws/generate")
async def websocket_generate(websocket: WebSocket):
    """
    TRUE STREAMING WebSocket endpoint using csm-streaming's generate_stream
    Audio chunks are sent AS they're being generated by the model
    """
    await websocket.accept()
    logger.info("ðŸ”— WebSocket client connected")

    try:
        # Receive generation request
        data = await websocket.receive_text()
        request = json.loads(data)

        text = request.get("text", "")
        speaker_id = request.get("speaker_id", 0)

        logger.info(f"ðŸ“ Generating for: {text[:50]}...")
        logger.info(f"ðŸŽ¤ Speaker ID: {speaker_id}")

        if not text:
            await websocket.send_json({"error": "No text provided"})
            await websocket.close()
            return

        chunks_sent = 0
        total_samples = 0
        
        logger.info("ðŸŽµ Starting TRUE streaming generation (chunks sent as generated)...")
        
        # TRUE STREAMING: Use generate_stream which yields chunks as model generates
        for audio_chunk in generator.generate_stream(
            text=text,
            speaker=speaker_id,
            context=[],  # Can add context segments for voice cloning
            max_audio_length_ms=30_000  # Max 30 seconds
        ):
            chunks_sent += 1
            total_samples += len(audio_chunk)
            
            # Convert tensor to int16 PCM if needed
            if torch.is_tensor(audio_chunk):
                audio_chunk = audio_chunk.cpu()
                if audio_chunk.dtype == torch.float32 or audio_chunk.dtype == torch.float16:
                    audio_chunk = (audio_chunk * 32767).to(torch.int16)
                audio_bytes = audio_chunk.numpy().tobytes()
            else:
                audio_bytes = audio_chunk.tobytes()
            
            # Send chunk immediately
            await websocket.send_bytes(audio_bytes)
            
            if chunks_sent == 1:
                logger.info(f"âš¡ First chunk sent (TRUE STREAMING - model still generating)")
            
            # Log every 10 chunks
            if chunks_sent % 10 == 0:
                logger.info(f"ðŸ“¤ Sent {chunks_sent} chunks ({total_samples} samples)")

        # Signal completion
        await websocket.send_json({"done": True})
        logger.info(f"âœ… Streaming complete: {chunks_sent} chunks, {total_samples} samples")
        logger.info(f"ðŸ“Š Estimated audio duration: {total_samples / generator.sample_rate:.2f}s")

    except WebSocketDisconnect:
        logger.info("ðŸ”Œ Client disconnected")
    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        try:
            await websocket.send_json({"error": str(e)})
        except:
            pass
    finally:
        try:
            await websocket.close()
        except:
            pass


# Keep REST API for backwards compatibility
class InputData(BaseModel):
    text: str
    speaker_id: int = 0


@app.post("/generate")
async def generate_audio(data: InputData):
    """REST API endpoint (legacy - slower than WebSocket)"""
    try:
        logger.info(f"ðŸŽ¤ REST API: Generating audio for: {data.text[:50]}...")
        
        file_id = str(uuid.uuid4())
        output_path = OUTPUT_DIR / f"{file_id}.wav"

        # Generate with streaming enabled internally for better performance
        import torchaudio
        audio = generator.generate(
            text=data.text,
            speaker=data.speaker_id,
            context=[],
            max_audio_length_ms=30_000,
            stream=True  # Internal streaming optimization
        )

        torchaudio.save(str(output_path), audio.unsqueeze(0).cpu(), generator.sample_rate)
        logger.info(f"âœ… Audio saved to: {output_path}")

        return {
            "message": "Audio generated successfully",
            "file": f"audio/{file_id}.wav"
        }

    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        return {"error": str(e)}, 500


@app.get("/audio/{filename}")
async def get_audio(filename: str):
    """Serve audio files"""
    file_path = OUTPUT_DIR / filename

    if not file_path.exists():
        return {"error": "File not found"}, 404

    return FileResponse(
        path=str(file_path),
        media_type="audio/wav",
        filename=filename
    )


@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "device": "cuda",
        "cuda": torch.cuda.is_available(),
        "model_loaded": generator is not None,
        "streaming": "true_realtime",
        "implementation": "csm-streaming by davidbrowne17",
        "rtf": "0.28x on RTX 4090",
        "endpoints": {
            "websocket": "/ws/generate",
            "rest": "/generate"
        }
    }


if __name__ == "__main__":
    import uvicorn
    logger.info("ðŸš€ Starting Sesame TTS Server (TRUE STREAMING with csm-streaming) on port 8000...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
